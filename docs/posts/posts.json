[
  {
    "path": "posts/statcan_2023-03-07/",
    "title": "CanSim Package Introduction",
    "description": "A new post by hswerdfe",
    "author": [
      {
        "name": "Howard Swerdfeger",
        "url": "https://hswerdfe.github.io/docs/"
      }
    ],
    "date": "2023-03-20",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\nShare on, \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nStatsCan releases a Ton of data tables, which are readily available and easy to download. In the R world the cansim package can be used a wrapper around these data.\r\nFirstly the cansim package can be installed with:\r\n\r\n\r\n install.packages('cansim')\r\n\r\n\r\nThen Loaded with:\r\n\r\n\r\nlibrary(cansim)\r\n\r\n\r\n\r\n\r\n\r\nOnce Loaded we can find a data table of interest by searching the data table of interest by keyword. In this case we use farm income :\r\n\r\n\r\nsearch_result <- \r\n  cansim::search_cansim_cubes(search_term) |>\r\n  dplyr::mutate(date_length  = difftime(cubeEndDate,cubeStartDate )) |> \r\n  dplyr::filter(cubeEndDate > '2020-01-01') |> \r\n  dplyr::arrange(dplyr::desc(date_length)) \r\n\r\n\r\ncansimId <- search_result$cansimId[[1]]\r\n\r\nsearch_result |> \r\n  utils::head(1) |>\r\n  dplyr::mutate_all(as.character)  |> \r\n  tidyr::pivot_longer(tidyr::everything())  |> \r\n  kableExtra::kable()\r\n\r\n\r\nname\r\n\r\n\r\nvalue\r\n\r\n\r\ncansim_table_number\r\n\r\n\r\n32-10-0052\r\n\r\n\r\ncubeTitleEn\r\n\r\n\r\nNet farm income\r\n\r\n\r\ncubeTitleFr\r\n\r\n\r\nRevenu agricole net\r\n\r\n\r\nproductId\r\n\r\n\r\n32100052\r\n\r\n\r\ncansimId\r\n\r\n\r\n002-0009\r\n\r\n\r\ncubeStartDate\r\n\r\n\r\n1926-01-01\r\n\r\n\r\ncubeEndDate\r\n\r\n\r\n2021-01-01\r\n\r\n\r\nreleaseTime\r\n\r\n\r\n2022-11-28 13:30:00\r\n\r\n\r\narchived\r\n\r\n\r\nFALSE\r\n\r\n\r\nsubjectCode\r\n\r\n\r\n32\r\n\r\n\r\nsurveyCode\r\n\r\n\r\n3473\r\n\r\n\r\nfrequencyCode\r\n\r\n\r\n12\r\n\r\n\r\ncorrections\r\n\r\n\r\n\r\n\r\ndimensionNameEn\r\n\r\n\r\nGeography, Income components\r\n\r\n\r\ndimensionNameFr\r\n\r\n\r\nGéographie, Catégories de revenus\r\n\r\n\r\nsurveyEn\r\n\r\n\r\nNet Farm Income\r\n\r\n\r\nsurveyFr\r\n\r\n\r\nRevenu agricole net\r\n\r\n\r\nsubjectEn\r\n\r\n\r\nAgriculture and food\r\n\r\n\r\nsubjectFr\r\n\r\n\r\nAgriculture and food\r\n\r\n\r\ndate_length\r\n\r\n\r\n34699\r\n\r\n\r\nUsing the Cansim ID given 002-0009 we can Meta Information about the data table with :\r\n\r\n\r\nmeta_raw <- cansim::get_cansim_cube_metadata(cansimId) |> janitor::clean_names()\r\n\r\nmeta_raw |> \r\n  utils::head(1) |>\r\n  dplyr::mutate_all(as.character)  |> \r\n  tidyr::pivot_longer(tidyr::everything())  |> \r\n  kableExtra::kable()\r\n\r\n\r\nname\r\n\r\n\r\nvalue\r\n\r\n\r\nproduct_id\r\n\r\n\r\n32-10-0052\r\n\r\n\r\ncansim_id\r\n\r\n\r\n002-0009\r\n\r\n\r\ncube_title_en\r\n\r\n\r\nNet farm income\r\n\r\n\r\ncube_title_fr\r\n\r\n\r\nRevenu agricole net\r\n\r\n\r\ncube_start_date\r\n\r\n\r\n1926-01-01\r\n\r\n\r\ncube_end_date\r\n\r\n\r\n2021-01-01\r\n\r\n\r\nnb_series_cube\r\n\r\n\r\n138\r\n\r\n\r\nnb_datapoints_cube\r\n\r\n\r\n8309\r\n\r\n\r\narchive_status_code\r\n\r\n\r\n2\r\n\r\n\r\narchive_status_en\r\n\r\n\r\nCURRENT - a cube available to the public and that is current\r\n\r\n\r\narchive_status_fr\r\n\r\n\r\nACTIF - un cube qui est disponible au public et qui est toujours mise a jour\r\n\r\n\r\nsubject_code\r\n\r\n\r\n320204\r\n\r\n\r\nsurvey_code\r\n\r\n\r\n3473\r\n\r\n\r\ndimension\r\n\r\n\r\n1,Geography,Géographie,FALSE,1,Canada,Canada,11124,1,0,2016,0,2,1,Newfoundland and Labrador,Terre-Neuve-et-Labrador,10,1,2,2016,0,3,1,Prince Edward Island,Île-du-Prince-Édouard,11,1,2,2016,0,4,1,Nova Scotia,Nouvelle-Écosse,12,1,2,2016,0,5,1,New Brunswick,Nouveau-Brunswick,13,1,2,2016,0,6,1,Quebec,Québec,24,1,2,2016,0,7,1,Ontario,Ontario,35,1,2,2016,0,8,1,Manitoba,Manitoba,46,1,2,2016,0,9,1,Saskatchewan,Saskatchewan,47,1,2,2016,0,10,1,Alberta,Alberta,48,1,2,2016,0,11,1,British Columbia,Colombie-Britannique,59,1,2,2016,0,2,Income components,Catégories de revenus,TRUE,10,Cash receipts, total,Recettes monétaires totales,0,81,11,Operating expenses after rebates,Dépenses d’exploitation après remise,0,81,12,11,Net cash income,Revenu net comptant,0,81,5,Income-in-kind,Revenu en nature,0,81,13,Depreciation charges,Dépenses d’amortissement,0,81,8,13,Realized net income,Revenu net réalisé,0,81,1,Gross income, total,Revenu brut, total,1,81,2,Value of inventory change,Valeur de la variation des stocks,0,81,9,2,Net income, total,Revenu net total,0,81,3,2,Realized gross income,Revenu brut réalisé,1,81,4,3,Cash receipts from farm products,Recettes monétaires provenant des produits agricoles,1,81,6,3,Supplementary payments,Paiements supplémentaires,1,81,7,Operating expenses and depreciation charges,Dépenses d’exploitation et d’amortissement,1,81\r\n\r\n\r\nrelease_time\r\n\r\n\r\n2022-12-06 11:00:00\r\n\r\n\r\nAnd the actual data with:\r\n\r\n\r\ndat_raw <- cansim::get_cansim(cansimId) |> \r\n  janitor::clean_names()\r\n\r\ndat <- \r\n  dat_raw %>% \r\n  dplyr::mutate_if(~(purrr::is_character(.) & dplyr::n_distinct(.) <=50), as.factor)\r\n\r\nskimr::skim(dat)\r\n\r\n\r\nTable 1: Data summary\r\n\r\n\r\nName\r\n\r\n\r\ndat\r\n\r\n\r\nNumber of rows\r\n\r\n\r\n8309\r\n\r\n\r\nNumber of columns\r\n\r\n\r\n21\r\n\r\n\r\n_______________________\r\n\r\n\r\n\r\n\r\nColumn type frequency:\r\n\r\n\r\n\r\n\r\ncharacter\r\n\r\n\r\n3\r\n\r\n\r\nDate\r\n\r\n\r\n1\r\n\r\n\r\nfactor\r\n\r\n\r\n15\r\n\r\n\r\nnumeric\r\n\r\n\r\n2\r\n\r\n\r\n________________________\r\n\r\n\r\n\r\n\r\nGroup variables\r\n\r\n\r\nNone\r\n\r\nVariable type: character\r\n\r\nskim_variable\r\n\r\n\r\nn_missing\r\n\r\n\r\ncomplete_rate\r\n\r\n\r\nmin\r\n\r\n\r\nmax\r\n\r\n\r\nempty\r\n\r\n\r\nn_unique\r\n\r\n\r\nwhitespace\r\n\r\n\r\nref_date\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\n0\r\n\r\n\r\n96\r\n\r\n\r\n0\r\n\r\n\r\nvector\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n0\r\n\r\n\r\n138\r\n\r\n\r\n0\r\n\r\n\r\ncoordinate\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n3\r\n\r\n\r\n5\r\n\r\n\r\n0\r\n\r\n\r\n138\r\n\r\n\r\n0\r\n\r\nVariable type: Date\r\n\r\nskim_variable\r\n\r\n\r\nn_missing\r\n\r\n\r\ncomplete_rate\r\n\r\n\r\nmin\r\n\r\n\r\nmax\r\n\r\n\r\nmedian\r\n\r\n\r\nn_unique\r\n\r\n\r\ndate\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1926-07-01\r\n\r\n\r\n2021-07-01\r\n\r\n\r\n1974-07-01\r\n\r\n\r\n96\r\n\r\nVariable type: factor\r\n\r\nskim_variable\r\n\r\n\r\nn_missing\r\n\r\n\r\ncomplete_rate\r\n\r\n\r\nordered\r\n\r\n\r\nn_unique\r\n\r\n\r\ntop_counts\r\n\r\n\r\ngeo\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n11\r\n\r\n\r\nNov: 813, Can: 800, Sas: 800, Alb: 799\r\n\r\n\r\ndguid\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n11\r\n\r\n\r\n201: 813, 201: 800, 201: 800, 201: 799\r\n\r\n\r\nuom\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n1\r\n\r\n\r\nDol: 8309\r\n\r\n\r\nuom_id\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n1\r\n\r\n\r\n81: 8309\r\n\r\n\r\nscalar_factor\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n1\r\n\r\n\r\ntho: 8309\r\n\r\n\r\nscalar_id\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n1\r\n\r\n\r\n3: 8309\r\n\r\n\r\nstatus\r\n\r\n\r\n8309\r\n\r\n\r\n0.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n0\r\n\r\n\r\n:\r\n\r\n\r\nsymbol\r\n\r\n\r\n8309\r\n\r\n\r\n0.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n0\r\n\r\n\r\n:\r\n\r\n\r\nterminated\r\n\r\n\r\n6288\r\n\r\n\r\n0.24\r\n\r\n\r\nFALSE\r\n\r\n\r\n1\r\n\r\n\r\nt: 2021\r\n\r\n\r\ndecimals\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n1\r\n\r\n\r\n0: 8309\r\n\r\n\r\ngeo_uid\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n11\r\n\r\n\r\n12: 813, 111: 800, 47: 800, 46: 799\r\n\r\n\r\nhierarchy_for_geo\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n11\r\n\r\n\r\n1.4: 813, 1: 800, 1.9: 800, 1.1: 799\r\n\r\n\r\nclassification_code_for_income_components\r\n\r\n\r\n8309\r\n\r\n\r\n0.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n0\r\n\r\n\r\n:\r\n\r\n\r\nhierarchy_for_income_components\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n13\r\n\r\n\r\n13.: 1011, 2: 1011, 2.9: 1011, 5: 1011\r\n\r\n\r\nincome_components\r\n\r\n\r\n0\r\n\r\n\r\n1.00\r\n\r\n\r\nFALSE\r\n\r\n\r\n13\r\n\r\n\r\nVal: 1011, Net: 1011, Inc: 1011, Rea: 1011\r\n\r\nVariable type: numeric\r\n\r\nskim_variable\r\n\r\n\r\nn_missing\r\n\r\n\r\ncomplete_rate\r\n\r\n\r\nmean\r\n\r\n\r\nsd\r\n\r\n\r\np0\r\n\r\n\r\np25\r\n\r\n\r\np50\r\n\r\n\r\np75\r\n\r\n\r\np100\r\n\r\n\r\nhist\r\n\r\n\r\nvalue\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1044984\r\n\r\n\r\n4060323\r\n\r\n\r\n-7586081\r\n\r\n\r\n11909\r\n\r\n\r\n76265\r\n\r\n\r\n506988\r\n\r\n\r\n83190530\r\n\r\n\r\n▇▁▁▁▁\r\n\r\n\r\nval_norm\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1044983702\r\n\r\n\r\n4060323043\r\n\r\n\r\n-7586081000\r\n\r\n\r\n11909000\r\n\r\n\r\n76265000\r\n\r\n\r\n506988000\r\n\r\n\r\n83190530000\r\n\r\n\r\n▇▁▁▁▁\r\n\r\n\r\nWe can see that there are several Income Components that we can look at:\r\n\r\n\r\n  dat |>\r\n  count(hierarchy_for_income_components, income_components) |>\r\n  tidyr::separate(col = hierarchy_for_income_components, into = paste0('h_', 1:3), convert = TRUE, remove  = FALSE) |>\r\n  select(-n) |> \r\n  rename(Hierarchy := hierarchy_for_income_components) |>\r\n  dplyr::arrange(h_1, h_2, h_3) |> \r\n  kableExtra::kable()\r\n\r\n\r\nHierarchy\r\n\r\n\r\nh_1\r\n\r\n\r\nh_2\r\n\r\n\r\nh_3\r\n\r\n\r\nincome_components\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nGross income, total\r\n\r\n\r\n2.3.4\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\nCash receipts from farm products\r\n\r\n\r\n2.3.6\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n6\r\n\r\n\r\nSupplementary payments\r\n\r\n\r\n2.3\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\nNA\r\n\r\n\r\nRealized gross income\r\n\r\n\r\n2.9\r\n\r\n\r\n2\r\n\r\n\r\n9\r\n\r\n\r\nNA\r\n\r\n\r\nNet income, total\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nValue of inventory change\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nIncome-in-kind\r\n\r\n\r\n7\r\n\r\n\r\n7\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nOperating expenses and depreciation charges\r\n\r\n\r\n10\r\n\r\n\r\n10\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nCash receipts, total\r\n\r\n\r\n11.12\r\n\r\n\r\n11\r\n\r\n\r\n12\r\n\r\n\r\nNA\r\n\r\n\r\nNet cash income\r\n\r\n\r\n11\r\n\r\n\r\n11\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nOperating expenses after rebates\r\n\r\n\r\n13.8\r\n\r\n\r\n13\r\n\r\n\r\n8\r\n\r\n\r\nNA\r\n\r\n\r\nRealized net income\r\n\r\n\r\n13\r\n\r\n\r\n13\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nDepreciation charges\r\n\r\n\r\nWe can easily look at the time series of Value of inventory change\r\n\r\n\r\np_d <- \r\n  dat |>\r\n  dplyr::filter(geo == 'Canada') |>\r\n  dplyr::select(date, income_components, value, hierarchy_for_income_components) |>\r\n  #dplyr::count(hierarchy_for_income_components, income_components)\r\n  tidyr::separate(col = hierarchy_for_income_components, into = paste0('h_', 1:3), convert = TRUE) |>\r\n  dplyr::arrange(date, h_1, h_2, h_3) |>\r\n  dplyr::filter(h_1 == 2) #|>\r\n  #dplyr::filter(is.na(h_3)) \r\n\r\n\r\ntheme_set(theme_minimal() +   \r\n        theme(panel.grid.major = element_blank(), \r\n        panel.grid.minor = element_blank(),\r\n        axis.text.x=element_blank(),\r\n        axis.text.y=element_blank(),\r\n        plot.title=element_text(size=25, hjust=0.5, face=\"bold\", colour=\"grey\", vjust=-1),\r\n        plot.subtitle = element_text(size=15, hjust=0.5, face=\"bold\", colour=\"grey\", vjust=-1),        \r\n        plot.caption =element_text(size=18, hjust=0.5, face=\"italic\", color=\"black\")\r\n        ))\r\n\r\n\r\np_d_ends <- \r\n  p_d |> \r\n  group_by(income_components) |>\r\n  #filter(date == max(date) | date == min(date) | value == min(value) | value == max(value))  |>\r\n  filter(date == max(date) | date == min(date) )  |>\r\n  mutate(lbl = glue::glue('{date}\\n${format(round(value),  big.mark   = \",\", digits = 3)}'))\r\n  #mutate(lbl = glue::glue('{income_components}\\n{date}\\n${format(round(value),  big.mark = \",\", digits = 3)}'))\r\n\r\n\r\nmid_range_value <- function(x){\r\n  mean(c(max(x), min(x)))\r\n}\r\n\r\np_d_lbl <- \r\n  p_d |> \r\n  group_by(income_components) |>\r\n  summarise(value = mid_range_value(value), date = mid_range_value(date))\r\n  \r\np_d_hline <- \r\n  p_d |> \r\n  group_by(income_components) |>\r\n  filter(max(value) > 0 & min(value) < 0) |>\r\n  ungroup()\r\n\r\n\r\np <- \r\n  p_d |> \r\n  ggplot(aes(x = date, y = value, color = income_components)) +\r\n  geom_text(data = p_d_lbl, mapping = aes(label = income_components), alpha = 0.5, size = 7) +\r\n  geom_point() +\r\n  geom_smooth() +\r\n  geom_text(data = p_d_ends, mapping = aes(label = lbl, fill = income_components), alpha = 0.5) +\r\n  #ggplot2::scale_y_log10(labels = scales::dollar_format(prefix=\"$\")) +\r\n  #ggplot2::scale_x_continuous() + \r\n  ggplot2::scale_x_date(date_breaks = '10 year', date_labels = '%Y') +\r\n  ggplot2::guides(color = FALSE, fill= FALSE) +\r\n  ggplot2::geom_hline(data = p_d_hline, mapping = aes(yintercept = 0), color = 'red', linetype = \"dashed\") +\r\n  ggplot2::facet_wrap(~income_components, scales = 'free_y', ncol = 1) +\r\n  labs(x = '', y = '', title = meta_raw$cube_title_en) +\r\n  theme(\r\n  strip.background = element_blank(),\r\n  strip.text.x = element_blank()\r\n  )\r\n#p\r\nggplotly(p)\r\n\r\n\r\n\r\nWe do a quick search for a income_components that has a large geographical variation we find\r\n\r\n\r\nsimple_models <- \r\n  dat |> \r\n  group_by(income_components, geo) |> \r\n  group_modify(~ broom::tidy(lm(value ~ date, data = .x))) |>\r\n  filter(term == 'date') |>\r\n  ungroup() \r\n\r\n\r\nvariation_across_geo <-\r\n  simple_models |> \r\n  group_by(income_components) |> \r\n  summarise(estimate_sd = sd(estimate),\r\n            estimate_rng = max(estimate)- min(estimate)) |> \r\n  arrange(desc(estimate_sd))\r\n\r\n\r\nvariation_across_geo |>\r\n    kableExtra::kable()\r\n\r\n\r\nincome_components\r\n\r\n\r\nestimate_sd\r\n\r\n\r\nestimate_rng\r\n\r\n\r\nCash receipts, total\r\n\r\n\r\n971.9935522\r\n\r\n\r\n3390.820929\r\n\r\n\r\nOperating expenses after rebates\r\n\r\n\r\n780.2543943\r\n\r\n\r\n2725.725726\r\n\r\n\r\nNet cash income\r\n\r\n\r\n193.2701312\r\n\r\n\r\n665.095205\r\n\r\n\r\nDepreciation charges\r\n\r\n\r\n112.3889962\r\n\r\n\r\n391.986964\r\n\r\n\r\nGross income, total\r\n\r\n\r\n76.4585980\r\n\r\n\r\n254.544545\r\n\r\n\r\nRealized gross income\r\n\r\n\r\n74.4456463\r\n\r\n\r\n247.790012\r\n\r\n\r\nCash receipts from farm products\r\n\r\n\r\n73.6189327\r\n\r\n\r\n244.950606\r\n\r\n\r\nOperating expenses and depreciation charges\r\n\r\n\r\n50.5067495\r\n\r\n\r\n166.907665\r\n\r\n\r\nRealized net income\r\n\r\n\r\n47.6196352\r\n\r\n\r\n161.210619\r\n\r\n\r\nNet income, total\r\n\r\n\r\n47.3885855\r\n\r\n\r\n160.854911\r\n\r\n\r\nSupplementary payments\r\n\r\n\r\n2.0176029\r\n\r\n\r\n7.045422\r\n\r\n\r\nIncome-in-kind\r\n\r\n\r\n0.8948958\r\n\r\n\r\n3.134577\r\n\r\n\r\nValue of inventory change\r\n\r\n\r\n0.7301864\r\n\r\n\r\n2.571054\r\n\r\n\r\nTake the measurement with the most difference deviation across geo codes, this would be Cash receipts, total\r\n\r\n\r\nlibrary(geofacet)\r\nincome_components_curr <- variation_across_geo |> head(1) |> pull(income_components)\r\n\r\ngeo_est <- \r\n  simple_models |>\r\n  filter(income_components == income_components_curr) |>\r\n  select(geo, estimate) |>\r\n  arrange(desc(estimate))\r\n  \r\n\r\n\r\n\r\np_d <- \r\n  dat |>\r\n  filter(income_components == income_components_curr) |>\r\n  select(date, income_components, value, geo) |>\r\n  left_join(geo_est, by = join_by(geo)) |>\r\n  filter(geo != 'Canada')  |>\r\n  mutate(geo = fct_reorder(geo, -estimate)) \r\n\r\n\r\n  \r\n\r\n\r\np_d_lbl <- \r\n  p_d |> \r\n  group_by(geo) |>\r\n  summarise(value = mid_range_value(value), date = mid_range_value(date), estimate = unique(estimate)) |>\r\n  left_join(\r\n          ca_prov_grid1 |>\r\n          mutate(name = factor(x = name,levels = levels(p_d$geo))) |> \r\n          as_tibble() |>\r\n          filter(!is.na(name)), \r\n          by = join_by(geo == name )\r\n        ) |>\r\n  mutate(lbl = code)\r\n  #mutate(lbl = paste0(code, '(', round(estimate, 0),')'))\r\n\r\n  \r\n\r\n\r\n\r\np_d_ends <- \r\n  p_d |> \r\n  group_by(geo) |>\r\n  #filter(date == max(date) | date == min(date) | value == min(value) | value == max(value))  |>\r\n  filter(date == max(date) | date == min(date) )  |>\r\n  mutate(lbl = glue::glue('{date}\\n${format(round(value),  big.mark   = \",\", digits = 3)}'))\r\n  #mutate(lbl = glue::glue('{income_components}\\n{date}\\n${format(round(value),  big.mark = \",\", digits = 3)}'))\r\n\r\n\r\np <- \r\np_d |> \r\n  ggplot(aes(x = date, y = value, color = geo)) +\r\n  geom_point() + \r\n  geom_smooth() + \r\n  geom_text(data = p_d_lbl, mapping = aes(label = lbl), alpha = 0.5, size = 12) +\r\n  geom_text(data = p_d_ends, mapping = aes(label = lbl), alpha = 0.5) +\r\n  # facet_geo(~ geo, \r\n  #           grid = \"ca_prov_grid1\"#, \r\n  #           #scales = 'free_y' \r\n  #           ) +\r\n  facet_wrap(\r\n    ~ geo, ncol = 3\r\n  ) +\r\n  guides(fill= FALSE, color = FALSE) +\r\n  labs(x='', y = '', title = income_components_curr) +\r\n  theme(\r\n  strip.background = element_blank(),\r\n  strip.text.x = element_blank()\r\n  )\r\n\r\nggplotly(p)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/statcan_2023-03-07/distill-preview.png",
    "last_modified": "2023-03-20T23:30:13-04:00",
    "input_file": {}
  },
  {
    "path": "posts/entity_resolution_2021-04-17/",
    "title": "Entity Resolution for Deduplication of data",
    "description": "A new post by hswerdfe",
    "author": [
      {
        "name": "Howard Swerdfeger",
        "url": "https://hswerdfe.github.io/docs/"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\r\nMotivation\r\nData quality sucks, and we have to match records togeather.\r\nLoad Libraries\r\n\r\n\r\nlibrary(knitr)\r\nlibrary(tidyverse)\r\nlibrary(janitor)\r\nlibrary(lubridate)\r\nlibrary(fuzzyjoin) # for common mispellings\r\n#library(maps) # for city Names, but I don't want to override purrr:map\r\nlibrary(babynames) # for first names\r\nlibrary(lexicon) # for Last names\r\nlibrary(reclin) # for de-duplication\r\nlibrary(phonics) # for soundex\r\nlibrary(plotROC) # for ROC curves\r\nlibrary(AUC)# for AUC calculations curves\r\nlibrary(magrittr) #extract2\r\nlibrary(igraph) # Neigbourhood determination\r\nlibrary(lemon)\r\nlibrary(snakecase)\r\ntheme_set(theme_minimal())\r\nknit_print.data.frame <- lemon_print\r\n\r\nset.seed(as.integer(as.Date(\"2021-04-09\")))\r\n\r\n\r\n\r\nGenerate Random Dataset\r\n\r\n\r\ng_num_entities = 72\r\ng_num_dup_max = 12\r\ng_prob_error = 0.05\r\ng_prob_miss = 0.05\r\n\r\n\r\n\r\nGenerate a dataset with:\r\n72 people in it\r\neach person will be duplicated between 1 and 12 times\r\neach cell have a 0.05 chance of being NA\r\neach character in each string has a 0.05 chance of being altered\r\nSome Utility Functions\r\n\r\n\r\n#'\r\n#' get a single character out of a string\r\n#'\r\n#' s2c(i = 1, \"data Doo Doo\")\r\ns2c <- function(i, str, ...){\r\n  substr(x = str, start = i,stop =  i, ...)\r\n}\r\n\r\n#'\r\n#' change a string to a vector \r\n#'\r\n#'example:\r\n#'  map_s2c(\"data Doo Doo\")\r\nmap_s2c<- function(str, ... ){\r\n  purrr::map(1:nchar(str), s2c, str = str, ...) %>% unlist()\r\n}\r\n\r\n\r\n#'\r\n#' Randomly edit some string\r\n#'\r\n#' random_edit(\"billy bob thorton\")\r\nrandom_edit <- function(str, prob_char = g_prob_error, sub_in = letters){\r\n  if_else(\r\n    sample(x = c(T, F), size = nchar(str), prob = c(prob_char,1-prob_char), replace = TRUE),\r\n    sample(x = sub_in, size =  nchar(str)),\r\n    map_s2c(str)\r\n  ) %>% paste0(collapse = \"\")\r\n}\r\n\r\n\r\n\r\n#'\r\n#' Generate a base set of entities \r\n#'\r\n#' generate_entities(10)\r\ngenerate_entities <- function(num_entities = g_num_entities, max_rep = g_num_dup_max){\r\n\r\n  dts_for_day_of_year = sample(seq(as.Date(\"1900-01-01\"), as.Date(\"1901-12-31\"),by = \"day\"), size = num_entities, replace = TRUE)\r\n\r\n    tibble(\r\n      first_name = \r\n        babynames::babynames %>% \r\n        group_by(name) %>% summarise(n = sum(n, na.rm = TRUE)) %>% \r\n        sample_n(size = num_entities, replace = TRUE, weight = n) %>% \r\n        pull(name),\r\n      middle_name = \r\n        babynames::babynames %>% \r\n        group_by(name) %>% summarise(n = sum(n, na.rm = TRUE)) %>% \r\n        sample_n(size = num_entities, replace = TRUE, weight = n) %>% \r\n        pull(name) ,     \r\n      last_name = \r\n        lexicon::freq_last_names %>% \r\n        clean_names() %>%\r\n        sample_n(size = num_entities, replace = TRUE, weight = prop) %>%\r\n        pull(surname),\r\n      city = \r\n        maps::world.cities %>% \r\n        clean_names() %>%\r\n        filter(country_etc ==\"USA\") %>% \r\n        sample_n(size = num_entities, replace = TRUE, weight = pop) %>% pull(name), \r\n      dob_year = \r\n        babynames::babynames  %>% \r\n        group_by(year) %>% \r\n        summarise(n = sum(n, na.rm = TRUE)) %>%\r\n        sample_n(size = num_entities, replace = TRUE, weight = n) %>% \r\n        pull(year),\r\n      dob_month = month(dts_for_day_of_year),\r\n      dob_day = day(dts_for_day_of_year)\r\n    ) %>% \r\n    mutate(dob = ISOdate(dob_year, dob_month, dob_day)) %>% \r\n    #select(-dob_year, -dob_month, -dob_day) %>% \r\n    mutate(., key = 1:nrow(.)) %>%\r\n    mutate(n = sample(1:max_rep, size = num_entities, replace = TRUE))  \r\n  \r\n}\r\n\r\n\r\n\r\nCreate Sample dataset\r\nIn this Dataset every record has a name a city and a date of birth.\r\nAdditionally it has a key we will use that later to figure out which record is which.\r\nWe also generate n which is how many times we will duplicate that record and make random edits to simulate noise.\r\n\r\n\r\nentities <- generate_entities()\r\nentities %>% sample_n(7)\r\n\r\n\r\n# A tibble: 7 x 10\r\n  first_name middle_name last_name city     dob_year dob_month dob_day\r\n  <chr>      <chr>       <chr>     <chr>       <dbl>     <dbl>   <int>\r\n1 Patrick    Veva        Mcdonald  New York     1959        10      26\r\n2 Gemma      Ricki       Padilla   Houston      1989         6      10\r\n3 Joshua     Kenneth     Anderson  Los Ang~     2016        12      21\r\n4 Alyssa     Lillian     James     San Jose     1991         6      24\r\n5 Sherri     Jaret       Jackson   Tucson       1943         7      29\r\n6 Charles    Roger       Jensen    Los Ang~     1950        10      17\r\n7 Thiago     Lindsay     Johnson   Charles~     2007         5       7\r\n# ... with 3 more variables: dob <dttm>, key <int>, n <int>\r\n\r\nNoisy Data\r\nMake the data noisy, by making random edits to 0.05 and setting 0.05 of random cells to NA. notice how the names clearly don’t look as clean.\r\n\r\n\r\nnoisy <- \r\n  entities %>% \r\n  uncount(weights = n) %>% \r\n  #select(-key) %>% \r\n  #head(5000) %>% \r\n  mutate(first_name = purrr::map(first_name, random_edit) %>% unlist()) %>%\r\n  mutate(middle_name = purrr::map(middle_name, random_edit) %>% unlist()) %>%  \r\n  mutate(last_name = purrr::map(last_name, random_edit) %>% unlist()) %>%\r\n  mutate(city = purrr::map(city, random_edit) %>% unlist()) %>% \r\n  mutate(dob_year = purrr::map(dob_year, random_edit, sub_in = as.character(0:9)) %>% unlist() %>% as.integer()) %>%\r\n  mutate(dob_month = purrr::map(dob_month, random_edit, sub_in = as.character(1:12)) %>% unlist() %>% as.integer()) %>%\r\n  mutate(dob_day = purrr::map(dob_day, random_edit, sub_in = as.character(1:31)) %>% unlist() %>% as.integer()) %>%\r\n  mutate(dob = ISOdate(dob_year, dob_month, dob_day))   %>%\r\n  select(-dob_year, -dob_month, -dob_day) %>%\r\n  mutate(.,first_name = if_else(sample(x = c(T,F), size = nrow(.), replace = T, prob = c(g_prob_miss, 1-g_prob_miss)),\r\n                               as.character(NA), first_name)) %>%\r\n  mutate(.,last_name = if_else(sample(x = c(T,F), size = nrow(.), replace = T, prob = c(g_prob_miss, 1-g_prob_miss)),\r\n                                 as.character(NA), last_name)) %>%  \r\n  mutate(.,city = if_else(sample(x = c(T,F), size = nrow(.), replace = T, prob = c(g_prob_miss, 1-g_prob_miss)),\r\n                                as.character(NA), city)) %>%  \r\n   mutate(.,dob = if_else(sample(x = c(T,F), size = nrow(.), replace = T, prob = c(g_prob_miss, 1-g_prob_miss)),\r\n                          ISOdate(99, 99, 99), dob)) \r\n\r\nnoisy %>% sample_n(7)\r\n\r\n\r\n# A tibble: 7 x 6\r\n  first_name middle_name last_name city      dob                   key\r\n  <chr>      <chr>       <chr>     <chr>     <dttm>              <int>\r\n1 Darlene    Doris       Jawssen   New Brau~ 1950-06-15 12:00:00    68\r\n2 Carolyn    Samuel      Agustin   Newton    1943-06-06 12:00:00     6\r\n3 Jeremy     Hayden      Daniels   Schenect~ 1952-11-04 12:00:00    34\r\n4 Pclly      Mckexzie    randy     Riverside 1970-03-01 12:00:00     9\r\n5 Sherki     Jarkt       eackson   Tucson    1943-07-29 12:00:00    31\r\n6 Alyssa     Lillian     James     San Jose  1991-06-24 12:00:00    70\r\n7 Jamie      Ariana      Wriyht    <NA>      1998-01-04 12:00:00    16\r\n\r\nLook at duplicates of one key\r\n\r\n\r\nkey_of_interest <-\r\n  noisy %>%\r\n  count(key, sort = T) %>%\r\n  slice(as.integer(g_num_entities/2)) %>%\r\n  pull(key)\r\n\r\nnoisy %>%\r\n  filter(key == key_of_interest)\r\n\r\n\r\n# A tibble: 6 x 6\r\n  first_name middle_name last_name city      dob                   key\r\n  <chr>      <chr>       <chr>     <chr>     <dttm>              <int>\r\n1 Txtry      vanzel      Waluh     Jackoonv~ 1974-02-05 12:00:00    32\r\n2 Terry      Daniel      Waxsh     Jacksonv~ 1974-02-05 12:00:00    32\r\n3 qerry      Daniel      Walsh     Jadzsmnv~ 1974-02-05 12:00:00    32\r\n4 Terry      faniel      Walsk     Japzsonv~ 1974-02-05 12:00:00    32\r\n5 Terry      Daninl      Walsh     Jacksonv~ 1974-03-05 12:00:00    32\r\n6 Txrry      Danieg      <NA>      Jacksonv~ 1974-02-05 12:00:00    32\r\n\r\nEach column will give us some indication that it is the same person, but each column by it self will not be enough to tell us, so we will look at multiple columns. There are various metrics we can look at\r\nAdd soundex\r\n\r\n\r\nnoisy <- \r\n  noisy %>% \r\n  select(-key, -dob) %>% \r\n  mutate_all(phonics::soundex) %>% \r\n  rename_all(~paste0(.x, \"_soundex\")) %>% \r\n  bind_cols(noisy, .)\r\n\r\nnoisy %>% sample_n(7)\r\n\r\n\r\n# A tibble: 7 x 10\r\n  first_name middle_name last_name city      dob                   key\r\n  <chr>      <chr>       <chr>     <chr>     <dttm>              <int>\r\n1 Patricia   Serena      Dunn      Saint Pe~ 1963-08-13 12:00:00    65\r\n2 Tom        Caitlin     Brooks    Enid      1951-03-04 12:00:00    11\r\n3 Hattie     Cameron     Regmlado  Fairfield 2007-10-11 12:00:00     2\r\n4 Raymond    Margaret    Mason     Huntingt~ 1620-07-30 12:00:00    30\r\n5 mubrles    Royer       Jensen    Los Ange~ 1950-10-17 12:00:00    20\r\n6 Jonathan   nary        Caron     Houston   2008-12-16 12:00:00    47\r\n7 Carolyn    Samuwl      Agustin   Newton    1943-06-06 12:00:00     6\r\n# ... with 4 more variables: first_name_soundex <chr>,\r\n#   middle_name_soundex <chr>, last_name_soundex <chr>,\r\n#   city_soundex <chr>\r\n\r\nPairs of values\r\n\r\n\r\n#columns to do comparisons on\r\ncols_4_simmilarity <- noisy %>% select(-key) %>% colnames()\r\ncols_4_simmilarity\r\n\r\n\r\n[1] \"first_name\"          \"middle_name\"         \"last_name\"          \r\n[4] \"city\"                \"dob\"                 \"first_name_soundex\" \r\n[7] \"middle_name_soundex\" \"last_name_soundex\"   \"city_soundex\"       \r\n\r\nblocking\r\nblocking for any one variable is the same\r\nBlocking is how you limit the computational complexity of what you are doing, here I am blocking on any one variable being identical, which seems reasonable as every column could be wrong with some small probability.\r\n\r\n\r\n  #\r\n  # blocking pairs with at least on cell, any cell identical\r\n  #\r\np1 <- \r\n  noisy %>% \r\n  select(-key) %>% \r\n  colnames() %>% \r\n  map_dfr(~pair_blocking(noisy, noisy, blocking_var=.x, large = F))\r\n\r\n\r\n\r\n#\r\n# pairs with no blocking (this is much longer)\r\n#\r\npa <- pair_blocking(x= noisy, y = noisy, large = F)\r\n\r\n\r\n#moving forward with any one column being same for computational reasons\r\np <- pa#p1\r\np\r\n\r\n\r\nSimple blocking\r\n  No blocking used.\r\n  First data set:  429 records\r\n  Second data set: 429 records\r\n  Total number of pairs: 184 041 pairs\r\n\r\nShowing first 20 pairs:\r\n    x y\r\n1   1 1\r\n2   2 1\r\n3   3 1\r\n4   4 1\r\n5   5 1\r\n6   6 1\r\n7   7 1\r\n8   8 1\r\n9   9 1\r\n10 10 1\r\n11 11 1\r\n12 12 1\r\n13 13 1\r\n14 14 1\r\n15 15 1\r\n16 16 1\r\n17 17 1\r\n18 18 1\r\n19 19 1\r\n20 20 1\r\n\r\nAbove p1 has 46271 rows, while pa has 184041 rows. this is on 429 records and 72 people, and this discrepency gets larger as our number of records get larger. Here I am using Pa and p1 for potential pairs\r\n\r\n\r\n#'\r\n#' Custom Date compare function\r\n#'\r\ndate_comp <- function(date_part){\r\n    function(x, y) {\r\n        if (!missing(y)) {\r\n          if (date_part == \"m\"){\r\n            x %>% month() == y %>% month()\r\n          }else if(date_part == \"d\"){\r\n            x %>% day() == y %>% day()\r\n          }else if(date_part == \"y\"){\r\n            abs(x %>% year() - y %>% year())\r\n          }else{\r\n            0\r\n          }\r\n        }\r\n        else {\r\n            (x > threshold) & !is.na(x)\r\n        }\r\n    }\r\n}\r\n#date_comp\r\n\r\n\r\n\r\nsimilarty metrics\r\nWe now have pairs to compare we look at how close individual cells in matching pairs are to each other, to do this we generate some comparisons. So here we generate 4 separate ways of comparing the strings across 3 different models. All the models will share soundex as a potential matching method. then each model will specialize in either jaro winkler, Longest Common Substring, or Jaccard simmilarity.\r\n\r\n\r\n#########################\r\n#soundex columns should be identical\r\nfunc_list_by_col <- \r\n  list(first_name_soundex = identical(), \r\n       middle_name_soundex = identical(),\r\n       last_name_soundex = identical(), \r\n       city_soundex = identical())\r\n\r\n########################\r\n# The library does not seem to support multiple comparitors on the same column at the same time so we do this, for the next best thing\r\ndefault_comparator_list <- \r\n  list(jw = jaro_winkler(), \r\n       lcs = lcs(),\r\n       jaccard = jaccard()\r\n      )\r\n\r\n############################\r\n# generate similarities for all the matching pairs\r\np_s <- \r\n  default_comparator_list %>%\r\n  map(function(func){\r\n    compare_pairs(pairs = p, \r\n                  by = cols_4_simmilarity,\r\n                  comparators = func_list_by_col,\r\n                  default_comparator = func)\r\n  })\r\nnames(p_s) <- names(default_comparator_list)\r\n\r\n\r\n\r\nObserving the simmilarities\r\nHere we sample from the pairs with similarities p_s to see that we now have many numerical values indicating how close each x y pair is along various metrics, higher numbers in general mean more likely to be a match as does TRUE in the binary columns.\r\n\r\n\r\nps_combined <- p_s %>% bind_rows(.id = 'model_name') \r\nps_combined %>% sample_n(5) %>% as_tibble()\r\n\r\n\r\n# A tibble: 5 x 12\r\n  model_name     x     y first_name middle_name last_name  city    dob\r\n  <chr>      <int> <int>      <dbl>       <dbl>     <dbl> <dbl>  <dbl>\r\n1 jw           323   104      0.483       0.421    NA     0.519  0.826\r\n2 jaccard       77   296      0           0         0     0     NA    \r\n3 jw           130   115      0.442       0.556     0.625 0.448  0.782\r\n4 lcs          399   377      0.333       0.75     NA     0.308 NA    \r\n5 jw           281    28      0           0.448     0     0.423  0.815\r\n# ... with 4 more variables: first_name_soundex <lgl>,\r\n#   middle_name_soundex <lgl>, last_name_soundex <lgl>,\r\n#   city_soundex <lgl>\r\n\r\nTrue Pairs\r\nSummation (simplest possible)\r\nFrom here we try to go from potential pairs to true pairs. The easiest thing to do is sum across all the numeric columns, and say that higher number as more likely to be a match.\r\n\r\n\r\n###############################\r\n# add similarities\r\np_s <- \r\n  p_s %>% \r\n  map(score_simsum, var = \"simpl_sum\")\r\n\r\nps_combined <- p_s %>% bind_rows(.id = 'model_name') \r\nps_combined %>%\r\n  ggplot(aes(x = simpl_sum, color = model_name)) + \r\n  geom_density() +\r\n  labs(color = \"Model\", x = \"Total Simmilarity Score\", y = \"Density\", title = \"Density of total scores of potential pairs by model\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n# default_comparator_list <- \r\n#   list(jw = jaro_winkler(), \r\n#        lcs = lcs(),\r\n#        jaccard = jaccard()\r\n#       )\r\n# p_s <- \r\n#   default_comparator_list %>%\r\n#   map(function(func){\r\n#     compare_pairs(pairs = p, \r\n#                   by = cols_4_simmilarity,\r\n#                   comparators = func_list_by_col,\r\n#                   default_comparator = func)\r\n#   })\r\n# names(p_s) <- names(default_comparator_list)\r\n\r\n\r\n\r\nEM algorithm\r\nThe EM Algorithm from the 2007 book Data Quality and Record Linkage Techniques generates the probability that each column contains information about weather the record pair is or is not a match\r\n\r\n\r\nem_s <- \r\n  p_s %>% \r\n  map(problink_em)\r\nem_s \r\n\r\n\r\n$jw\r\nM- and u-probabilities estimated by the EM-algorithm:\r\n            Variable M-probability U-probability\r\n          first_name     0.4261385  1.239424e-09\r\n         middle_name     0.6986917  4.067077e-09\r\n           last_name     0.4094873  9.044788e-16\r\n                city     0.4096737  3.182302e-03\r\n                 dob     0.4764895  1.187817e-03\r\n  first_name_soundex     0.4841342  4.286314e-03\r\n middle_name_soundex     0.7627071  8.458971e-04\r\n   last_name_soundex     0.4743486  9.135294e-04\r\n        city_soundex     0.3774117  5.705523e-04\r\n\r\nMatching probability: 0.02480617.\r\n\r\n$lcs\r\nM- and u-probabilities estimated by the EM-algorithm:\r\n            Variable M-probability U-probability\r\n          first_name     0.6758350  6.074621e-06\r\n         middle_name     0.7881897  7.229890e-03\r\n           last_name     0.7160520  2.463114e-14\r\n                city     0.7615701  7.337231e-03\r\n                 dob     0.7425143  5.190125e-03\r\n  first_name_soundex     0.6031999  4.284000e-03\r\n middle_name_soundex     0.7067278  5.810422e-03\r\n   last_name_soundex     0.5981588  7.827820e-04\r\n        city_soundex     0.4508693  9.752591e-04\r\n\r\nMatching probability: 0.01989889.\r\n\r\n$jaccard\r\nM- and u-probabilities estimated by the EM-algorithm:\r\n            Variable M-probability U-probability\r\n          first_name     0.4363094  1.486481e-09\r\n         middle_name     0.6933837  1.558471e-08\r\n           last_name     0.4192609  7.779157e-16\r\n                city     0.4171563  3.170574e-03\r\n                 dob     0.4705465  5.812155e-04\r\n  first_name_soundex     0.4952393  4.294945e-03\r\n middle_name_soundex     0.7576207  1.423543e-03\r\n   last_name_soundex     0.4852706  9.229072e-04\r\n        city_soundex     0.3868390  5.598046e-04\r\n\r\nMatching probability: 0.02422578.\r\n\r\n\r\n\r\np_s <-\r\n  p_s %>% \r\n  map(function(p_i){\r\n    score_problink(p_i, var = \"em_weight\")\r\n  })    \r\n\r\n\r\nps_combined <- p_s %>% bind_rows(.id = 'model_name') \r\nps_combined %>%\r\n  ggplot(aes(x = em_weight, color = model_name)) + \r\n  geom_density() +\r\n  labs(color = \"Model\", x = \"EM Weight Score\", y = \"Density\", title = \"Density of total scores of potential pairs by model\")\r\n\r\n\r\n\r\n\r\nresults and metrics\r\nGenerate a results the is_same column tells us if they are truely from the same person.\r\n\r\n\r\np_results <- \r\n  map2_dfr(names(p_s), p_s, function(nm, p){\r\n    p$def_func = nm\r\n    p %>% as_tibble()\r\n  }) \r\n\r\nnoisy_kr <- \r\n  noisy %>% \r\n  mutate(., row = 1:nrow(.)) %>% \r\n  select(\"key\", \"row\")\r\n\r\np_results <- \r\n  p_results %>% \r\n  left_join(noisy_kr %>% set_names(c(\"key_x\", \"x\")), by = \"x\") %>% \r\n  left_join(noisy_kr %>% set_names(c(\"key_y\", \"y\")), by = \"y\")  %>% \r\n  mutate(is_same = key_x == key_y)\r\np_results %>% summary()\r\n\r\n\r\n       x             y         first_name     middle_name    \r\n Min.   :  1   Min.   :  1   Min.   :0.00    Min.   :0.0000  \r\n 1st Qu.:108   1st Qu.:108   1st Qu.:0.00    1st Qu.:0.0000  \r\n Median :215   Median :215   Median :0.15    Median :0.1667  \r\n Mean   :215   Mean   :215   Mean   :0.21    Mean   :0.2191  \r\n 3rd Qu.:322   3rd Qu.:322   3rd Qu.:0.43    3rd Qu.:0.4365  \r\n Max.   :429   Max.   :429   Max.   :1.00    Max.   :1.0000  \r\n                             NA's   :60048                   \r\n   last_name          city            dob         first_name_soundex\r\n Min.   :0.00    Min.   :0.00    Min.   :0.11     Mode :logical     \r\n 1st Qu.:0.00    1st Qu.:0.00    1st Qu.:0.40     FALSE:483138      \r\n Median :0.15    Median :0.18    Median :0.56     TRUE :8937        \r\n Mean   :0.21    Mean   :0.23    Mean   :0.58     NA's :60048       \r\n 3rd Qu.:0.43    3rd Qu.:0.42    3rd Qu.:0.78                       \r\n Max.   :1.00    Max.   :1.00    Max.   :1.00                       \r\n NA's   :55176   NA's   :55176   NA's   :109755                     \r\n middle_name_soundex last_name_soundex city_soundex   \r\n Mode :logical       Mode :logical     Mode :logical  \r\n FALSE:541224        FALSE:489960      FALSE:245088   \r\n TRUE :10899         TRUE :6987        TRUE :5475     \r\n                     NA's :55176       NA's :301560   \r\n                                                      \r\n                                                      \r\n                                                      \r\n   simpl_sum        em_weight         def_func        \r\n Min.   :0.0000   Min.   : -9.408   Length:552123     \r\n 1st Qu.:0.4913   1st Qu.: -5.990   Class :character  \r\n Median :1.2121   Median : -5.070   Mode  :character  \r\n Mean   :1.3285   Mean   : -3.373                     \r\n 3rd Qu.:1.8183   3rd Qu.: -3.085                     \r\n Max.   :9.0000   Max.   :107.512                     \r\n                                                      \r\n     key_x           key_y        is_same       \r\n Min.   : 1.00   Min.   : 1.00   Mode :logical  \r\n 1st Qu.:16.00   1st Qu.:16.00   FALSE:541530   \r\n Median :35.00   Median :35.00   TRUE :10593    \r\n Mean   :34.66   Mean   :34.66                  \r\n 3rd Qu.:51.00   3rd Qu.:51.00                  \r\n Max.   :72.00   Max.   :72.00                  \r\n                                                \r\n\r\np_results\r\n\r\n\r\n# A tibble: 552,123 x 17\r\n       x     y first_name middle_name last_name  city    dob\r\n   <int> <int>      <dbl>       <dbl>     <dbl> <dbl>  <dbl>\r\n 1     1     1      1           1         1     1      1    \r\n 2     2     1      1           0.778     0.867 0.917 NA    \r\n 3     3     1      1           0.889     0.867 1      1    \r\n 4     4     1      1           0.889     0.867 1      1    \r\n 5     5     1      1           0.778     0.867 1      0.919\r\n 6     6     1      1           0.889     0.867 1      1    \r\n 7     7     1      0.889       0.889     0.867 0.917  0.965\r\n 8     8     1      0.889       0.889     0.867 0.833  1    \r\n 9     9     1      1           0.889     0.733 0.917  1    \r\n10    10     1      1           0.889     0.867 1      1    \r\n# ... with 552,113 more rows, and 10 more variables:\r\n#   first_name_soundex <lgl>, middle_name_soundex <lgl>,\r\n#   last_name_soundex <lgl>, city_soundex <lgl>, simpl_sum <dbl>,\r\n#   em_weight <dbl>, def_func <chr>, key_x <int>, key_y <int>,\r\n#   is_same <lgl>\r\n\r\n\r\n\r\np_results %>% \r\n  select(-x, -y) %>% \r\n  mutate_if(is.logical, as.double) %>% \r\n  pivot_longer(!def_func) %>%\r\n  mutate(def_func = if_else(grepl(\"soundex\",name), \"soundex\", def_func)) %>% \r\n  mutate(name = if_else(grepl(\"soundex\",name), gsub(\"_soundex\", \"\", name), name)) %>% \r\n  filter(!name %in% c('key_x','key_y','is_same')) %>% \r\n  #mutate(name = snakecase::to_title_case(name)) %>% \r\n  ggplot(aes(x = value, color = def_func)) + \r\n  geom_density() + \r\n#  facet_grid(cols = vars(name), rows = vars(def_func),  scales = \"free\")\r\n  facet_wrap(~ name, scales = \"free\") + \r\n  labs(color = \"model\", title = \"similarity metrics distribution across each column and each model\")\r\n\r\n\r\n\r\n\r\n\r\n\r\np_results %>% \r\n  select(simpl_sum, em_weight, def_func, is_same) %>%\r\n  pivot_longer(., cols=c(\"simpl_sum\", \"em_weight\")) %>% \r\n  mutate(mdl_nm = paste0(def_func, \"_\", name )) \r\n\r\n\r\n# A tibble: 1,104,246 x 5\r\n   def_func is_same name       value mdl_nm      \r\n   <chr>    <lgl>   <chr>      <dbl> <chr>       \r\n 1 jw       TRUE    simpl_sum   8    jw_simpl_sum\r\n 2 jw       TRUE    em_weight 101.   jw_em_weight\r\n 3 jw       TRUE    simpl_sum   4.56 jw_simpl_sum\r\n 4 jw       TRUE    em_weight  26.1  jw_em_weight\r\n 5 jw       TRUE    simpl_sum   6.76 jw_simpl_sum\r\n 6 jw       TRUE    em_weight  44.3  jw_em_weight\r\n 7 jw       TRUE    simpl_sum   6.76 jw_simpl_sum\r\n 8 jw       TRUE    em_weight  44.3  jw_em_weight\r\n 9 jw       TRUE    simpl_sum   5.56 jw_simpl_sum\r\n10 jw       TRUE    em_weight  31.1  jw_em_weight\r\n# ... with 1,104,236 more rows\r\n\r\nLook at Model Results\r\nLooking at the model results from the ROC perspective I can say, that clearly I did not make this problem hard enough, and well I am evaluating on my training set. Moving one\r\n\r\n\r\np_results %>% \r\n  select(simpl_sum, em_weight, def_func, is_same) %>%\r\n  pivot_longer(., cols=c(\"simpl_sum\", \"em_weight\")) %>% \r\n  mutate(mdl_nm = paste0(def_func, \"_\", name )) %>%\r\n  group_by(mdl_nm) %>% \r\n  mutate(auc = AUC::auc(AUC::roc(value , as.factor(is_same)))) %>% \r\n  ungroup() %>% \r\n  mutate(mdl_nm = paste0(\"(\", round(auc, 5), \") \", mdl_nm )) %>%\r\n  mutate(mdl_nm = fct_reorder(mdl_nm, auc)) %>%\r\n  ggplot(aes(d = is_same, m = value, color = mdl_nm)) + \r\n  geom_roc(n.cuts = 10) + \r\n  style_roc() + \r\n  labs()\r\n\r\n\r\n\r\n\r\n###Select Best Model\r\n\r\n\r\np_results %>% \r\n  select(simpl_sum, em_weight, def_func, is_same) %>%\r\n  pivot_longer(., cols=c(\"simpl_sum\", \"em_weight\")) %>% \r\n  mutate(mdl_nm = paste0(def_func, \"_\", name )) %>%\r\n  group_by(mdl_nm) %>% \r\n  mutate(auc = AUC::auc(AUC::roc(value , as.factor(is_same)))) %>% \r\n  ungroup() %>% \r\n  mutate(mdl_nm = paste0(\"(\", auc, \") \", mdl_nm )) %>%\r\n  mutate(mdl_nm = fct_reorder(mdl_nm, auc)) %>%\r\n  filter(auc == max(auc)) %>%\r\n  ggplot(aes(d = is_same, m = value, color = mdl_nm)) + \r\n  geom_roc(n.cuts = 20) + \r\n  style_roc() + \r\n  labs()\r\n\r\n\r\n\r\n\r\n\r\n\r\nbest_model_results <- \r\n  p_results %>% \r\n  select(simpl_sum, em_weight, def_func, is_same, x, y) %>%\r\n  pivot_longer(., cols=c(\"simpl_sum\", \"em_weight\")) %>% \r\n  mutate(mdl_nm = paste0(def_func, \"_\", name )) %>%\r\n  group_by(mdl_nm) %>% \r\n  mutate(auc = AUC::auc(AUC::roc(value , as.factor(is_same)))) %>% \r\n  ungroup() %>% \r\n  filter(auc == max(auc)) \r\nbest_model_results\r\n\r\n\r\n# A tibble: 184,041 x 8\r\n   def_func is_same     x     y name      value mdl_nm          auc\r\n   <chr>    <lgl>   <int> <int> <chr>     <dbl> <chr>         <dbl>\r\n 1 lcs      TRUE        1     1 em_weight  73.3 lcs_em_weight  1.00\r\n 2 lcs      TRUE        2     1 em_weight  17.8 lcs_em_weight  1.00\r\n 3 lcs      TRUE        3     1 em_weight  32.6 lcs_em_weight  1.00\r\n 4 lcs      TRUE        4     1 em_weight  32.6 lcs_em_weight  1.00\r\n 5 lcs      TRUE        5     1 em_weight  22.7 lcs_em_weight  1.00\r\n 6 lcs      TRUE        6     1 em_weight  32.6 lcs_em_weight  1.00\r\n 7 lcs      TRUE        7     1 em_weight  10.4 lcs_em_weight  1.00\r\n 8 lcs      TRUE        8     1 em_weight  12.7 lcs_em_weight  1.00\r\n 9 lcs      TRUE        9     1 em_weight  28.8 lcs_em_weight  1.00\r\n10 lcs      TRUE       10     1 em_weight  32.6 lcs_em_weight  1.00\r\n# ... with 184,031 more rows\r\n\r\n\r\nAnother way of looking at the data\r\n\r\n\r\ncut_quantiles <- function(x, by = 0.02) {\r\n  cut(x, breaks=unique(c(quantile(x, probs = seq(0, 1, by = by)))), \r\n      #labels=c(\"0-20\",\"20-40\",\"40-60\",\"60-80\",\"80-100\"), \r\n      include.lowest=TRUE)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nbest_model_results %>%\r\n  mutate(value_cut = cut_quantiles(value)) %>% \r\n  group_by(value_cut, is_same) %>%\r\n  summarise(n = n()) %>% ungroup() %>%\r\n  group_by(value_cut) %>% mutate(f = n/sum(n)) %>% ungroup() %>% \r\n  ggplot(aes(x = value_cut, y = f, fill = is_same)) + geom_col() + coord_flip()\r\n\r\n\r\n\r\n\r\nselect Threshold\r\n\r\n\r\nbest_model_results %>% \r\narrange(value) %>% \r\nmutate(f_cumsum_true=cumsum(is_same)/sum(is_same)) %>% \r\nmutate(f_cumsum_false=cumsum(!is_same)/sum(!is_same)) %>% \r\nrename(score:=value,\r\n       score_typ:=name) %>%\r\npivot_longer(cols = c(f_cumsum_true, f_cumsum_false)) %>%\r\nggplot(aes(x=score, y=value, color= name)) + geom_line()\r\n\r\n\r\n\r\n\r\n\r\n\r\nThreshold <- \r\n  best_model_results %>% \r\n  arrange(value) %>% \r\n  mutate(f_cumsum_true=cumsum(is_same)/sum(is_same)) %>% \r\n  mutate(f_cumsum_false=cumsum(!is_same)/sum(!is_same)) %>%\r\n  arrange(abs(f_cumsum_false - 0.9995)) %>%\r\n  slice(1) %>% pull(value)\r\nThreshold\r\n\r\n\r\n[1] 8.101735\r\n\r\n\r\n\r\nThreshold2 <- \r\n  best_model_results %>% \r\n  arrange(value) %>% \r\n  mutate(f_cumsum_true=cumsum(is_same)/sum(is_same)) %>% \r\n  mutate(f_cumsum_false=cumsum(!is_same)/sum(!is_same)) %>%\r\n  arrange(abs(f_cumsum_true - 0.75)) %>%\r\n  slice(1) %>% pull(value)\r\nThreshold2\r\n\r\n\r\n[1] 65.64459\r\n\r\nMake the Graph of duplicates\r\nEach node is a record\r\nEach edge is a duplicate as determined by the winning model and threashold\r\n\r\n\r\nThreshold\r\n\r\n\r\n[1] 8.101735\r\n\r\ng <- make_empty_graph(n = 0, directed=FALSE) +\r\n    vertices(paste0(\"N_\",1:nrow(noisy)))\r\n\r\n\r\nedges_vector <- \r\n  best_model_results %>% \r\n  filter(value >= Threshold) %>% \r\n  #filter(x != y) %>% \r\n  select(x, y) %>% #head(5) %>% \r\n  pmap(function(x, y){\r\n    c(x,y)\r\n  }) %>% unlist()\r\n\r\n#edges_vector\r\ngorder(g) %>% print()\r\n\r\n\r\n[1] 429\r\n\r\ngsize(g) %>% print()\r\n\r\n\r\n[1] 0\r\n\r\n#g\r\n\r\n\r\n\r\n\r\n\r\ng<-add_edges(g, edges_vector)\r\ngorder(g) %>% print()\r\n\r\n\r\n[1] 429\r\n\r\ngsize(g) %>% print()\r\n\r\n\r\n[1] 3517\r\n\r\ng\r\n\r\n\r\nIGRAPH 2ee511f UN-- 429 3517 -- \r\n+ attr: name (v/c)\r\n+ edges from 2ee511f (vertex names):\r\n [1] N_1--N_1  N_1--N_2  N_1--N_3  N_1--N_4  N_1--N_5  N_1--N_6 \r\n [7] N_1--N_7  N_1--N_8  N_1--N_9  N_1--N_10 N_1--N_11 N_1--N_2 \r\n[13] N_2--N_2  N_2--N_3  N_2--N_4  N_2--N_5  N_2--N_6  N_2--N_7 \r\n[19] N_2--N_8  N_2--N_9  N_2--N_10 N_2--N_11 N_1--N_3  N_2--N_3 \r\n[25] N_3--N_3  N_3--N_4  N_3--N_5  N_3--N_6  N_3--N_7  N_3--N_8 \r\n[31] N_3--N_9  N_3--N_10 N_3--N_11 N_1--N_4  N_2--N_4  N_3--N_4 \r\n[37] N_4--N_4  N_4--N_5  N_4--N_6  N_4--N_7  N_4--N_8  N_4--N_9 \r\n[43] N_4--N_10 N_4--N_11 N_1--N_5  N_2--N_5  N_3--N_5  N_4--N_5 \r\n+ ... omitted several edges\r\n\r\nGet disconected components of graph\r\n\r\n\r\ng_s <- decompose.graph(g)\r\ngi <- \r\n  g_s %>%\r\n  sample(x = ., size=1) %>% \r\n  extract2(1) \r\nprint(length(g_s))\r\n\r\n\r\n[1] 67\r\n\r\nprint(gi)\r\n\r\n\r\nIGRAPH 2f38396 UN-- 7 56 -- \r\n+ attr: name (v/c)\r\n+ edges from 2f38396 (vertex names):\r\n [1] N_100--N_100 N_100--N_100 N_100--N_101 N_100--N_101 N_101--N_101\r\n [6] N_101--N_101 N_100--N_102 N_100--N_102 N_101--N_102 N_101--N_102\r\n[11] N_102--N_102 N_102--N_102 N_100--N_103 N_100--N_103 N_101--N_103\r\n[16] N_101--N_103 N_102--N_103 N_102--N_103 N_103--N_103 N_103--N_103\r\n[21] N_100--N_104 N_100--N_104 N_101--N_104 N_101--N_104 N_102--N_104\r\n[26] N_102--N_104 N_103--N_104 N_103--N_104 N_104--N_104 N_104--N_104\r\n[31] N_100--N_105 N_100--N_105 N_101--N_105 N_101--N_105 N_102--N_105\r\n[36] N_102--N_105 N_103--N_105 N_103--N_105 N_104--N_105 N_104--N_105\r\n+ ... omitted several edges\r\n\r\n\r\n\r\ngwithn <- tibble(g=as.character(), n_nm=as.character(), n = as.integer())\r\n\r\nfor (i in 1:length(g_s)){\r\n  g_nm = paste0(\"g_\",i)\r\n  #print(g_nm)\r\n  g_i <- as_ids(V(g_s[[i]]))\r\n  for (i_n in g_i){\r\n    gwithn <- \r\n      gwithn %>% add_row(g = g_nm, n_nm = i_n, n = as.integer(gsub(x=i_n, pattern=\"N_\", replacement=\"\")))\r\n    #print()\r\n  }\r\n  \r\n}\r\ngwithn <- gwithn %>% distinct()\r\ngwithn\r\n\r\n\r\n# A tibble: 429 x 3\r\n   g     n_nm      n\r\n   <chr> <chr> <int>\r\n 1 g_1   N_1       1\r\n 2 g_1   N_2       2\r\n 3 g_1   N_3       3\r\n 4 g_1   N_4       4\r\n 5 g_1   N_5       5\r\n 6 g_1   N_6       6\r\n 7 g_1   N_7       7\r\n 8 g_1   N_8       8\r\n 9 g_1   N_9       9\r\n10 g_1   N_10     10\r\n# ... with 419 more rows\r\n\r\nAbove g is a group of records that are a person and n_nmis a record. For the fully disconnected graph we have predicted there to be 67 distinct people in 429 records. when there are in fact nrow(entities) people in the origional dataset\r\nbased on community ratther then fully disconnnected\r\n\r\n\r\ngwithn <- tibble(g=as.character(), n_nm=as.character(), n = as.integer())\r\n\r\ng_c <- fastgreedy.community(simplify(as.undirected(g)))\r\nwalk(1:length(g_c), function(i_c){\r\n  ig <- g_c[[i_c]]\r\n  #print(i_c)\r\n  g_nm = paste0(\"g_\", i_c)\r\n  walk(1:length(ig), function(i_n){\r\n    #print(ig[[i_n]])\r\n    n_nm <- ig[[i_n]]\r\n    gwithn <<- \r\n      gwithn %>% add_row(g = g_nm, \r\n                         n_nm = n_nm, \r\n                         n = as.integer(gsub(x=n_nm, pattern=\"N_\", replacement=\"\"))\r\n                         )\r\n  })\r\n})\r\ngwithn\r\n\r\n\r\n# A tibble: 429 x 3\r\n   g     n_nm      n\r\n   <chr> <chr> <int>\r\n 1 g_1   N_36     36\r\n 2 g_1   N_37     37\r\n 3 g_1   N_38     38\r\n 4 g_1   N_39     39\r\n 5 g_1   N_40     40\r\n 6 g_1   N_41     41\r\n 7 g_1   N_42     42\r\n 8 g_1   N_43     43\r\n 9 g_1   N_44     44\r\n10 g_1   N_45     45\r\n# ... with 419 more rows\r\n\r\nFor the community algorithm 67 distinct people in 429 records. when there are in fact nrow(entities) people in the origional dataset\r\nplot one random component\r\n\r\n\r\nplot(gi)\r\n\r\n\r\n\r\n\r\nPlot Whole Graph\r\n\r\n\r\nplot(g)\r\n\r\n\r\n\r\n\r\nExtract df of sub_graph with Node\r\nSo now we will summarize the data for later in a data frame.\r\n\r\n\r\nmost_common_names_concat <- function(x, sep = \"; \", n = 2){\r\n    x = as.character(x)\r\n    n2 = min(n, length(x))\r\n    tmp <- sort(table(x),decreasing=T)[1:n2] \r\n    paste0(names(tmp), \"(\",tmp, \")\") %>%  paste0(collapse=sep)\r\n}\r\nmost_common_names_vector <- function(x, n = 2){\r\n  n2 = min(n, length(x))\r\n  tmp <- sort(table(x),decreasing=T)[1:n2] \r\n  tmp2 <- as_tibble(tmp) \r\n  tmp_l <- as.list(tmp2$n)\r\n  names(tmp_l) <- tmp2$x\r\n  tmp_l\r\n}\r\nunique_list <- function(x){\r\n  x %>% table() %>% sort(decreasing=T) %>% names() %>% list()\r\n}\r\nconcat_vect_remove <- function(x, sep = \"; \"){\r\n  x_u <- x %>% unique()\r\n  x_u <- x_u[!is.na(x_u)]\r\n  paste0(x_u, collapse=sep)\r\n}\r\ncount_unique <-function(x){\r\n  x_u <- x %>% unique() \r\n  x_u <- x_u[!is.na(x_u)]\r\n  length(x_u)\r\n}\r\n\r\n\r\n\r\nSummarize\r\nNow we know all the miss-spellings of each persons names.\r\n\r\n\r\nde_noised <- \r\n  noisy %>%\r\n  mutate(., n = 1:nrow(.)) %>% \r\n  left_join(gwithn, by = \"n\") %>%\r\n  group_by(g) %>%\r\n  summarise(n = n(), \r\n           first_name = unique_list(first_name),\r\n           last_name = unique_list(last_name),\r\n           city = unique_list(city),\r\n           dob = unique_list(dob),\r\n           key_n = n_distinct(key),\r\n           key = unique_list(key)#,\r\n           #key = most_common_names_concat(key)\r\n           ) %>% ungroup()%>%\r\n            arrange(desc(key_n)) \r\n\r\nde_noised\r\n\r\n\r\n# A tibble: 67 x 8\r\n   g         n first_name last_name  city      dob      key_n key     \r\n   <chr> <int> <list>     <list>     <list>    <list>   <int> <list>  \r\n 1 g_11     30 <chr [8]>  <chr [11]> <chr [16~ <chr [8~     4 <chr [4~\r\n 2 g_27     27 <chr [10]> <chr [10]> <chr [12~ <chr [6~     4 <chr [4~\r\n 3 g_31     11 <chr [1]>  <chr [3]>  <chr [7]> <chr [4~     2 <chr [2~\r\n 4 g_1      12 <chr [3]>  <chr [8]>  <chr [5]> <chr [4~     1 <chr [1~\r\n 5 g_10     11 <chr [4]>  <chr [3]>  <chr [5]> <chr [1~     1 <chr [1~\r\n 6 g_12      9 <chr [3]>  <chr [4]>  <chr [4]> <chr [1~     1 <chr [1~\r\n 7 g_13      9 <chr [4]>  <chr [2]>  <chr [2]> <chr [1~     1 <chr [1~\r\n 8 g_14      9 <chr [5]>  <chr [4]>  <chr [7]> <chr [2~     1 <chr [1~\r\n 9 g_15      9 <chr [4]>  <chr [6]>  <chr [1]> <chr [4~     1 <chr [1~\r\n10 g_16      9 <chr [5]>  <chr [1]>  <chr [1]> <chr [2~     1 <chr [1~\r\n# ... with 57 more rows\r\n\r\nConclusion\r\nMake a form with validation checks! … Please…\r\nsessionInfo\r\n\r\n\r\nprint(R.version.string)\r\n\r\n\r\n[1] \"R version 4.0.4 (2021-02-15)\"\r\n\r\nsessionInfo(package = NULL)\r\n\r\n\r\nR version 4.0.4 (2021-02-15)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 18363)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_Canada.1252  LC_CTYPE=English_Canada.1252   \r\n[3] LC_MONETARY=English_Canada.1252 LC_NUMERIC=C                   \r\n[5] LC_TIME=English_Canada.1252    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n [1] snakecase_0.11.0 lemon_0.4.5      igraph_1.2.6    \r\n [4] magrittr_2.0.1   AUC_0.3.0        plotROC_2.2.1   \r\n [7] phonics_1.3.9    reclin_0.1.1     ldat_0.3.3      \r\n[10] Rcpp_1.0.6       lvec_0.2.2       lexicon_1.2.1   \r\n[13] babynames_1.0.1  fuzzyjoin_0.1.6  lubridate_1.7.10\r\n[16] janitor_2.1.0    forcats_0.5.1    stringr_1.4.0   \r\n[19] dplyr_1.0.5      purrr_0.3.4      readr_1.4.0     \r\n[22] tidyr_1.1.3      tibble_3.1.0     ggplot2_3.3.3   \r\n[25] tidyverse_1.3.0  knitr_1.31      \r\n\r\nloaded via a namespace (and not attached):\r\n [1] fs_1.5.0           httr_1.4.2         tools_4.0.4       \r\n [4] backports_1.2.1    bslib_0.2.4        utf8_1.2.1        \r\n [7] R6_2.5.0           DBI_1.1.1          colorspace_2.0-0  \r\n[10] withr_2.4.1        tidyselect_1.1.0   gridExtra_2.3     \r\n[13] downlit_0.2.1      compiler_4.0.4     cli_2.4.0         \r\n[16] rvest_1.0.0        xml2_1.3.2         labeling_0.4.2    \r\n[19] sass_0.3.1         scales_1.1.1       digest_0.6.27     \r\n[22] rmarkdown_2.7      stringdist_0.9.6.3 pkgconfig_2.0.3   \r\n[25] htmltools_0.5.1.1  dbplyr_2.1.0       highr_0.8         \r\n[28] maps_3.3.0         rlang_0.4.10       readxl_1.3.1      \r\n[31] rstudioapi_0.13    jquerylib_0.1.3    generics_0.1.0    \r\n[34] farver_2.1.0       jsonlite_1.7.2     distill_1.2       \r\n[37] munsell_0.5.0      fansi_0.4.2        lifecycle_1.0.0   \r\n[40] stringi_1.5.3      yaml_2.2.1         plyr_1.8.6        \r\n[43] grid_4.0.4         parallel_4.0.4     crayon_1.4.1      \r\n[46] lattice_0.20-41    haven_2.3.1        hms_1.0.0         \r\n[49] pillar_1.5.1       lpSolve_5.6.15     reprex_1.0.0      \r\n[52] glue_1.4.2         evaluate_0.14      data.table_1.14.0 \r\n[55] modelr_0.1.8       vctrs_0.3.7        cellranger_1.1.0  \r\n[58] gtable_0.3.0       assertthat_0.2.1   xfun_0.22         \r\n[61] syuzhet_1.0.6      broom_0.7.5        ellipsis_0.3.1    \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/entity_resolution_2021-04-17/distill-preview.png",
    "last_modified": "2023-03-07T12:29:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/mice_dates_2021-04-09/",
    "title": "Mice for Imputation of Observations With Consecutive Related Dates",
    "description": "A new post by hswerdfe",
    "author": [
      {
        "name": "Howard Swerdfeger",
        "url": "https://hswerdfe.github.io/docs/"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\r\nMotivation\r\nDataset with all of the following properties:\r\nObservations with many dates which are near each other\r\nMany cells with missing data\r\nBias in how the cells data are missing\r\nThink of a dataset with some of these as dates:\r\nInfection Date\r\nLab Test Date\r\nLab Result Date\r\nOnset Date\r\nReported Date\r\nRecovery Date\r\nLoad Libraries\r\nFirst load the libraries needed, mice is the library that actually does the imputation, while ambient is for creating a realistic looking fake dataset using perlin noise, and lemon is just for outputing tables.\r\n\r\n\r\nlibrary(tidyverse) # data manipulation\r\nlibrary(lubridate) # dates \r\nlibrary(mice) # imputation\r\nlibrary(ambient) # perlin noise\r\nlibrary(lemon) # for table output\r\nlibrary(ggpubr) #for ggarrange\r\nlibrary(ggridges)\r\n\r\nknit_print.data.frame <- lemon_print\r\ntheme_set(theme_minimal())\r\n\r\nset.seed(as.integer(as.Date(\"2021-04-09\")))\r\n\r\n\r\n\r\nMake some utility functions\r\nThese two function noise_perlin_1D_pos and sample_perlin are just used to help make the sample dataset we need, with dates that are missing but not missing at random.\r\n\r\n\r\n#'\r\n#' Returns 1D perlin Noise, and that noise is always above zero, vector length is size\r\n#' examples:\r\n#'    noise_perlin_1D_pos(50)\r\nnoise_perlin_1D_pos<- function(size, frequency = 0.007, ...){\r\n  pn <- noise_perlin(c(size, 1), frequency = frequency, ...)[,1]\r\n  pn - min(pn)  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n#'\r\n#' like base sample\r\n#' samples from x size times, with prob given by prob, which defaults to perlin noise\r\n#' \r\nsample_perlin <- function(x, size, replace = FALSE, ...,   \r\n                          prob = noise_perlin_1D_pos(length(x), ...)){\r\n  sample(x = x, size = size, prob = prob, replace = replace)\r\n}\r\n\r\n\r\n\r\nCreate Sample dataset\r\nSingle time series\r\nCreate a single timeseries, that serves a the basis for the rest of the dataset.\r\n\r\n\r\nn_rec = 10000\r\ns_dt = as.Date(\"2020-01-01\")\r\ne_dt = as.Date(\"2021-01-01\")\r\ndt_a <- sample_perlin(s_dt:e_dt, n_rec, replace = TRUE, frequency = 0.007) %>% as.Date(origin = as.Date(\"1970-01-01\")) %>% sort(\r\n  \r\n)\r\n\r\n\r\ntibble(dt_a) %>% \r\n  ggplot(aes(x = dt_a)) +\r\n  geom_density(color = \"red\", size = 1.5) +\r\n  labs(title = \"Density Plot of a base time series\", x = \"\", y = \"\")\r\n\r\n\r\n\r\n\r\nOther Time series\r\nAll other timeseries are base on the initial timeseries. We add factors in, but we don’t really use them.\r\n\r\n\r\nexposure_date <- dt_a + runif(n = n_rec, -21, -2) %>% round()\r\nlab_date <- exposure_date + runif(n = n_rec, 0, +28)%>% round()\r\nonset_date <- exposure_date + runif(n = n_rec, +2, +21)%>% round()\r\nreported_date <- lab_date + runif(n = n_rec, +5, +10)%>% round()\r\nrecovery_date <- onset_date + runif(n = n_rec, +2, +21)%>% round()\r\nfactor_a <- sample(c(\"a\",\"b\",\"c\",\"d\"), n_rec, replace = TRUE)\r\nfactor_b <- sample(c(\"z\",\"y\",\"x\",\"w\"), n_rec, replace = TRUE)\r\n\r\n\r\n\r\ndat_full <- \r\n  tibble(\r\n    exposure_date, lab_date, onset_date, reported_date, recovery_date, factor_a, factor_b\r\n  ) %>% \r\n  mutate(date_min = pmin(exposure_date, lab_date, onset_date, reported_date, recovery_date, na.rm = TRUE)) %>% \r\n  mutate(date_max = pmax(exposure_date, lab_date, onset_date, reported_date, recovery_date, na.rm = TRUE)) #%>% \r\n  #mutate(exposure_date = as.character(exposure_date))\r\n\r\ndat_full %>% sample_n(7)\r\n\r\n\r\nTable 1: Observations Dates are always close to each other.\r\nexposure_date\r\nlab_date\r\nonset_date\r\nreported_date\r\nrecovery_date\r\nfactor_a\r\nfactor_b\r\ndate_min\r\ndate_max\r\n2020-05-27\r\n2020-06-14\r\n2020-06-14\r\n2020-06-20\r\n2020-06-27\r\na\r\ny\r\n2020-05-27\r\n2020-06-27\r\n2019-12-20\r\n2020-01-07\r\n2020-01-04\r\n2020-01-13\r\n2020-01-16\r\nc\r\nx\r\n2019-12-20\r\n2020-01-16\r\n2020-04-07\r\n2020-04-23\r\n2020-04-25\r\n2020-04-30\r\n2020-05-10\r\nc\r\nz\r\n2020-04-07\r\n2020-05-10\r\n2020-05-04\r\n2020-05-17\r\n2020-05-15\r\n2020-05-25\r\n2020-05-30\r\na\r\ny\r\n2020-05-04\r\n2020-05-30\r\n2020-06-01\r\n2020-06-09\r\n2020-06-16\r\n2020-06-16\r\n2020-06-22\r\nb\r\ny\r\n2020-06-01\r\n2020-06-22\r\n2020-09-02\r\n2020-09-24\r\n2020-09-07\r\n2020-10-03\r\n2020-09-27\r\nc\r\nx\r\n2020-09-02\r\n2020-10-03\r\n2020-06-20\r\n2020-07-04\r\n2020-06-30\r\n2020-07-09\r\n2020-07-18\r\nd\r\nw\r\n2020-06-20\r\n2020-07-18\r\n\r\nRemove some values\r\n\r\n\r\nremove_noise <- noise_perlin_1D_pos(n_rec, pertubation = 'fractal',  frequency = 0.0001) \r\n\r\n\r\n\r\nremove_noise %>% \r\n  tibble(value = ., index = 1:length(.)) %>%\r\n  ggplot(aes(y = value, x = index)) + geom_line(color = \"blue\", size = 1.5) +\r\n  theme_void() + \r\n  labs(title = \"Probability density of cell being removed\")\r\n\r\n\r\n\r\n\r\nRandomly remove some values, but importantly the removal is not uniform across the whole dataset.\r\nHere the number removed is related to size for example date_b has about 1/3rd of its values removed.\r\n\r\n\r\nexposure_date[sample(x = 1:n_rec, size = n_rec/1.5, prob = remove_noise)] <- NA\r\nlab_date[sample(x = 1:n_rec, size = n_rec/2, prob = remove_noise)] <- NA\r\nonset_date[sample(x = 1:n_rec, size = 0.25*n_rec, prob = remove_noise)] <- NA\r\nreported_date[sample(x = 1:n_rec, size = n_rec/1000, prob = remove_noise)] <- NA\r\nrecovery_date[sample(x = 1:n_rec, size = 0.90*n_rec, prob = remove_noise)] <- NA\r\nfactor_a[sample(x = 1:n_rec, size = n_rec/3, prob = remove_noise)] <- NA\r\nfactor_b[sample(x = 1:n_rec, size = n_rec/3, prob = remove_noise)] <- NA\r\n\r\ndat <- \r\n  tibble(\r\n    exposure_date, lab_date, onset_date, reported_date, recovery_date, factor_a, factor_b\r\n  ) %>% \r\n  mutate(date_min = pmin(exposure_date, lab_date, onset_date, recovery_date, na.rm = TRUE)) %>% \r\n  mutate(date_max = pmax(exposure_date, lab_date, onset_date, recovery_date, na.rm = TRUE)) #%>% \r\n  #mutate(exposure_date = as.character(exposure_date))\r\n\r\ndat %>% sample_n(7)\r\n\r\n\r\nTable 2: Some cells are hidden or set to NA.\r\nexposure_date\r\nlab_date\r\nonset_date\r\nreported_date\r\nrecovery_date\r\nfactor_a\r\nfactor_b\r\ndate_min\r\ndate_max\r\nNA\r\n2020-11-13\r\nNA\r\n2020-11-21\r\nNA\r\nd\r\nz\r\n2020-11-13\r\n2020-11-13\r\nNA\r\nNA\r\n2020-09-28\r\n2020-10-19\r\nNA\r\nNA\r\ny\r\n2020-09-28\r\n2020-09-28\r\n2020-10-31\r\nNA\r\n2020-11-19\r\n2020-11-25\r\nNA\r\nd\r\nx\r\n2020-10-31\r\n2020-11-19\r\nNA\r\nNA\r\n2020-07-29\r\n2020-08-16\r\nNA\r\nb\r\nz\r\n2020-07-29\r\n2020-07-29\r\n2020-07-04\r\n2020-07-08\r\n2020-07-13\r\n2020-07-16\r\n2020-07-17\r\nc\r\nw\r\n2020-07-04\r\n2020-07-17\r\nNA\r\n2020-08-21\r\n2020-08-29\r\n2020-08-26\r\nNA\r\nb\r\nw\r\n2020-08-21\r\n2020-08-29\r\n2020-08-24\r\n2020-08-25\r\n2020-09-04\r\n2020-08-31\r\nNA\r\nd\r\nx\r\n2020-08-24\r\n2020-09-04\r\n\r\n\r\n\r\ndat %>% \r\n  select(everything()) %>%  # replace to your needs\r\n  summarise_all(funs(100*sum(is.na(.))/n())) %>% \r\n  pivot_longer(everything())\r\n\r\n\r\n# A tibble: 9 x 2\r\n  name          value\r\n  <chr>         <dbl>\r\n1 exposure_date  66.7\r\n2 lab_date       50  \r\n3 onset_date     25  \r\n4 reported_date   0.1\r\n5 recovery_date  90  \r\n6 factor_a       33.3\r\n7 factor_b       33.3\r\n8 date_min       12.9\r\n9 date_max       12.9\r\n\r\nCompare missing with full data\r\nThen compare the full dataset to the dataset with values removed. Note how badly some of the timeseries are represented, with the removed data. One of the worst is recovery_date. This is mainly because we removed 89 percent of the values.\r\n\r\n\r\nbind_rows(\r\ndat %>% mutate(full = \"Some Dates Removed\"),\r\ndat_full %>% mutate(full = \"Full Dataset\")\r\n) %>%\r\n  mutate_at(vars(matches(\"date\")), as.Date) %>% \r\n  select(matches(\"date\"), full) %>% \r\n  pivot_longer(cols = matches(\"date\")) %>% \r\n  count(full, name, value) %>%\r\n  filter(!is.na(value)) %>% \r\n  ggplot(aes(x = value, y = n, color = full)) + geom_line() + facet_wrap(vars(name)) + \r\n  labs(title = \"Example Dataset\", subtitle = \"some dates are removed\", y = \"Count of Data\", x = \"\", color = \"\") +\r\n  theme(legend.position=\"bottom\")\r\n\r\n\r\n\r\n\r\nDifference Between Dates\r\nOne thing we rely on is that the distribution of differences between the days is relatively consistent when data is removed, we can see that this is still the case.\r\nIf you have strong reason to suspect this is not the case, consider eliminating that date from the prediction.\r\n\r\n\r\nbind_rows(\r\n  dat %>% mutate(full = \"With Missing Data\"),\r\n  dat_full %>% mutate(full = \"Full Dataset\")\r\n) %>% \r\n  mutate(delta_onset_reported = onset_date- reported_date) %>%\r\n  ggplot(aes(x = delta_onset_reported, y = full, fill = full)) + \r\n  geom_density_ridges(alpha = 0.5) + \r\n  #facet_grid(rows = vars(full)) +\r\n  guides(fill = FALSE) + \r\n  labs(x = \"Days between Dates `e` and and `c`\")\r\n\r\n\r\n\r\n\r\nMain Imputation Function\r\nNext is the main function to restore the data in a target date column. What this does:\r\nwhere it can it shows finds the difference between the target date and all other dates\r\nuse mice to fill in the missing deltas to the target\r\ncalculate the target where needed using new deltas and helper dates\r\nI experimented a little with both my dataset and this fake data and found that predicting via mice on the delta of the dates seems to produce more reasonable answers then predicting the actual number via mice as often it seems a few dates will be way off with predicting mice on the dates directly. even if the dates are normalized.\r\nBut I did no formal experiment. This was just my intuition, as these were my first two attempts and they did not have accurate results.\r\n\r\n\r\n#'\r\n#' Uses Mice to predict the delta between 'other dates' and target date.\r\n#' When the 'target date' is missing estimate it from 'other dates'\r\n#' Then takes average of predictions of 'target dates', and returns a vector of 'target dates' \r\n#' with all data filled in....\r\n#' \r\n#' This method is useful when you have data with many dates\r\n#' \r\n#' \r\nfill_summarize_date_data <- function(\r\n                      dat,\r\n                      post_grouping_nm,\r\n                      predict_col_nm,\r\n                      col_dt_match_ptrn =\"date\",\r\n                      m = 3,\r\n                      maxit = 50,\r\n                      remove.collinear = FALSE, \r\n                      dt_origin_INTERNAL = as.Date(\"1970-01-01\"),\r\n                      simplify = TRUE,\r\n                      ...\r\n                    ){\r\n  n_rec = nrow(dat)\r\n  \r\n  \r\n  ###############################         \r\n  #Get just date columns             \r\n  dat_dt <- \r\n    dat %>% \r\n    select(matches(col_dt_match_ptrn)) %>%  \r\n    mutate_all(as.Date) %>%  \r\n    mutate_all(as.integer) \r\n  \r\n  \r\n  # Get the to and from columns\r\n  pd_to = dat_dt[[predict_col_nm]]\r\n  pd_from = dat_dt %>% select(-all_of(predict_col_nm))\r\n  pd_from_mat <- as.matrix(pd_from)\r\n  \r\n  ###################################\r\n  #do Mice on the deltas between columns\r\n  detlta_dt_mice <- \r\n  map_dfc(colnames(pd_from), function(nm){\r\n    pd_from[[nm]] - pd_to\r\n  }) %>% set_names(colnames(pd_from)) %>% \r\n    mice::mice(m = m,      #number of multiple Imputations\r\n         maxit = maxit,    #number of iterations\r\n         remove.collinear=remove.collinear, ...)\r\n  \r\n  \r\n  ############################\r\n  #\r\n  #Example:\r\n  #\r\n  # shows \"m\" estimate of the number of days between lab_date and target date, for each observation\r\n  #\r\n  # detlta_dt_mice$imp$lab_date\r\n  #\r\n  ####################################\r\n  \r\n  \r\n  ###########################\r\n  #get a bunch of predictions \r\n  #\r\n  # One prediction based on each \"helper column \"m\"\r\n  dat_comp <- \r\n    map_dfc(1:m, function(im){\r\n      \r\n      #get ith prediction\r\n      tmp <- mice::complete(data = detlta_dt_mice, im) %>% as_tibble()\r\n      \r\n\r\n      #in every helper column predict the target column when needed\r\n      #from the value and the delta\r\n      map_dfc(colnames(tmp), function(nm){\r\n        from_x = pd_from[[nm]]\r\n        dx = tmp[[nm]]\r\n        ifelse(is.na(pd_to),# Only use the prediction if we need to, otherwise use the recored value\r\n               from_x - dx,\r\n               pd_to\r\n        )\r\n        \r\n        \r\n      }) %>% \r\n        setNames(colnames(tmp)) %>%\r\n        mutate(., key = 1:nrow(.)) %>% \r\n        pivot_longer(., cols = matches(col_dt_match_ptrn)) %>%\r\n        group_by(key) %>%\r\n        summarise(value = mean(value, na.rm = TRUE)) %>% # AVERAGE each of the different types of dates\r\n        pull(value) %>%\r\n        as.Date(origin = dt_origin_INTERNAL)\r\n    }) %>% set_names(paste0(\"n_\", 1:m)) %>%\r\n    bind_cols(dat[predict_col_nm])\r\n  \r\n  \r\n    ##############################\r\n    #strip out other stuff and just return one vector averaging all the predicted dates  \r\n    dat_comp %>%\r\n      mutate(., tmp_key = 1:nrow(.)) %>% \r\n      pivot_longer(., cols = starts_with(\"n_\")) %>%\r\n      group_by(tmp_key) %>%\r\n      summarise(value = mean(value, na.rm = TRUE)) %>%\r\n      ungroup() %>% \r\n      pull(value) %>% round() #%>% class()\r\n}\r\n\r\n\r\n\r\nImputation (Prediction)\r\nNext do the prediction, important values are m and maxit which are both passed to mice. if you are interested in the individual entries I recommend increasing maxit and possibly m, at the cost of run time.\r\nHowever, if you are going to group by and count then a lower number for maxit is fine.\r\n\r\n\r\ndat_guess <- \r\n  dat %>% \r\n  mutate(., exposure_date_guess = fill_summarize_date_data(., \r\n                          predict_col_nm = \"exposure_date\", \r\n                          col_dt_match_ptrn =\"date\",\r\n                          m = 5, \r\n                          maxit = 100)\r\n                          )# %>% sample_n(20)\r\n\r\n\r\n\r\nNote:\r\ndate_e_guess is fully filled out while date_e is till missing some dates\r\ndate_e and date_e_guess are the same when they can be\r\ndate_e_guess seems reasonable given the other dates in the observations.\r\n\r\n\r\nbind_cols(\r\ndat_full %>% select(exposure_date),\r\ndat_guess %>% select(exposure_date, exposure_date_guess) %>% rename(exposure_date_missing := exposure_date)\r\n)%>% \r\n  mutate(guess_delta = exposure_date - exposure_date_guess) %>% \r\n  sample_n(10)\r\n\r\n\r\nTable 3: New column date_e_guess.\r\nexposure_date\r\nexposure_date_missing\r\nexposure_date_guess\r\nguess_delta\r\n2020-05-29\r\nNA\r\n2020-06-01\r\n-3 days\r\n2020-04-23\r\nNA\r\n2020-04-19\r\n4 days\r\n2020-07-23\r\n2020-07-23\r\n2020-07-23\r\n0 days\r\n2019-12-29\r\nNA\r\n2019-12-31\r\n-2 days\r\n2020-08-26\r\nNA\r\n2020-08-31\r\n-5 days\r\n2020-04-24\r\nNA\r\n2020-04-20\r\n4 days\r\n2020-01-12\r\nNA\r\n2020-01-25\r\n-13 days\r\n2020-07-03\r\n2020-07-03\r\n2020-07-03\r\n0 days\r\n2020-09-16\r\nNA\r\n2020-09-12\r\n4 days\r\n2020-06-25\r\n2020-06-25\r\n2020-06-25\r\n0 days\r\n\r\nLooking at the distribution of errors in the guesses we see that there is little systemic error. if we are worried about individual observations then bringing in the standard deviation would be needed but looking at aggregate data can require less precision at the individual level.\r\n\r\n\r\nbind_cols(\r\ndat_full %>% select(exposure_date),\r\ndat_guess %>% select(exposure_date, exposure_date_guess) %>% rename(exposure_date_missing := exposure_date)\r\n)%>% \r\n  filter(is.na(exposure_date_missing)) %>% \r\n  mutate(guess_delta = exposure_date - exposure_date_guess) %>% \r\n  count(guess_delta) %>%\r\n  ggplot(aes(x = guess_delta, y = n)) + geom_col(fill = \"light yellow\", alpha = 0.5, color = \"black\") + geom_vline(xintercept = 0, color = \"Black\", size = 1.5) +\r\n  labs(y = \"\", x = \"\", title = \"Distribution of Errors in Predicted Dates.\")\r\n\r\n\r\n\r\n\r\nCompare Plot\r\nNow we can see that predicting the missing dates in aggregate looks much better then just using the dates without imputation.\r\n\r\n\r\nbind_rows(\r\n  dat_full %>%\r\n    mutate_at(vars(matches(\"date\")), as.Date) %>% \r\n    select(matches(\"date\")) %>% \r\n    pivot_longer(.,cols = colnames(.)) %>% \r\n    mutate(full = \"full\") \r\n  ,\r\n  dat_guess %>% select(starts_with(\"exposure_date\")) %>%\r\n    pivot_longer(., cols = colnames(.)) %>%\r\n    mutate(full = \"missing\") \r\n) %>% \r\n  mutate(type = paste0(name,\"_\", full)) %>%\r\n  \r\n  count(type, value, sort = T) %>%# view()\r\n  filter(!is.na(value)) %>%\r\n  filter(grepl('exposure_date', type) ) %>% \r\n  ggplot(aes(x = value, y= n, color = type)) + geom_line() +\r\n  labs(title =\"Comparison of full timeseries, missing and imputed timeseries\", y = \"Number of observations\", x= \"Date\", color = \"\") +\r\n    theme(legend.position=\"bottom\")\r\n\r\n\r\n\r\n\r\n\r\n\r\ndat_comb <- \r\nbind_rows(\r\n  dat_full %>%\r\n    select(starts_with(\"exposure_date\")) %>% \r\n    pivot_longer(.,cols = colnames(.)) %>% \r\n    mutate(full = \"full\") \r\n  ,\r\n  dat_guess %>% select(starts_with(\"exposure_date\")) %>%\r\n    pivot_longer(., cols = colnames(.)) %>%\r\n    mutate(full = \"missing\") \r\n) %>% \r\n  mutate(type = paste0(name,\"_\", full)) %>% \r\n  count(type, value, sort = T) %>% \r\n  filter(!is.na(value)) %>% \r\n  pivot_wider(names_from = type, values_from = n) %>% \r\n  mutate(error_in_counts = exposure_date_full- exposure_date_guess_missing) %>%\r\n  mutate(error_in_counts_percent = error_in_counts/exposure_date_full)\r\n\r\n\r\ndat_comb$error_in_counts %>% quantile(probs = c(0.1,0.25,0.5,0.75,0.90), na.rm = TRUE)\r\n\r\n\r\n10% 25% 50% 75% 90% \r\n -7  -4   0   3   8 \r\n\r\np1 <-\r\ndat_comb %>%\r\n  ggplot(aes(x = value, y = error_in_counts)) + geom_line() +\r\n  labs(y =\"Error between Actual counts and predicted counts\", x = \"Date\", title = \"Errors Look Random Which is Good.\")\r\np2 <-\r\ndat_comb %>% \r\n  ggplot(aes(x = error_in_counts)) + \r\n  geom_density(fill = \"yellow\", color = \"black\", alpha = 0.15) + \r\n  geom_vline(xintercept = 0 , color = \"Black\", size = 1.5) +\r\n  #geom_boxplot(fill = \"yellow\", color = \"black\", alpha = 0.15) + \r\n  #geom_violin(fill = \"yellow\", color = \"black\", alpha = 0.15) + \r\n  labs(y = \"Density\", title = \"Distribution Error in Count\", x = \"Count\")\r\n\r\n\r\nggarrange(p1, p2)\r\n\r\n\r\n\r\n\r\nConclusion\r\nImpute your Dates! … Please…\r\n\r\n\r\n\r\n",
    "preview": "posts/mice_dates_2021-04-09/distill-preview.png",
    "last_modified": "2023-03-07T12:29:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/tt_2021-03-02/",
    "title": "Clustering of Tidy Tuesday Superbowl ads",
    "description": "A new post by hswerdfe",
    "author": [
      {
        "name": "Howard Swerdfeger",
        "url": "https://hswerdfe.github.io/docs/"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\r\nload libraries\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(plotly)\r\nlibrary(skimr)\r\nlibrary(snakecase)\r\nlibrary(janitor)\r\nlibrary(broom)\r\nlibrary(plotly)\r\nlibrary(htmlwidgets)\r\nlibrary(DT)\r\n\r\n#set the theme\r\ntheme_set(theme_minimal())\r\n\r\n\r\n\r\nRead in the data\r\nRead in and look at the data\r\n\r\n\r\nsb_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')\r\nskimr::skim(sb_raw)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\nsb_raw\r\nNumber of rows\r\n247\r\nNumber of columns\r\n25\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n10\r\nlogical\r\n7\r\nnumeric\r\n7\r\nPOSIXct\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nbrand\r\n0\r\n1.00\r\n3\r\n9\r\n0\r\n10\r\n0\r\nsuperbowl_ads_dot_com_url\r\n0\r\n1.00\r\n34\r\n120\r\n0\r\n244\r\n0\r\nyoutube_url\r\n11\r\n0.96\r\n43\r\n43\r\n0\r\n233\r\n0\r\nid\r\n11\r\n0.96\r\n11\r\n11\r\n0\r\n233\r\n0\r\nkind\r\n16\r\n0.94\r\n13\r\n13\r\n0\r\n1\r\n0\r\netag\r\n16\r\n0.94\r\n27\r\n27\r\n0\r\n228\r\n0\r\ntitle\r\n16\r\n0.94\r\n6\r\n99\r\n0\r\n228\r\n0\r\ndescription\r\n50\r\n0.80\r\n3\r\n3527\r\n0\r\n194\r\n0\r\nthumbnail\r\n129\r\n0.48\r\n48\r\n48\r\n0\r\n118\r\n0\r\nchannel_title\r\n16\r\n0.94\r\n3\r\n37\r\n0\r\n185\r\n0\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\nfunny\r\n0\r\n1\r\n0.69\r\nTRU: 171, FAL: 76\r\nshow_product_quickly\r\n0\r\n1\r\n0.68\r\nTRU: 169, FAL: 78\r\npatriotic\r\n0\r\n1\r\n0.17\r\nFAL: 206, TRU: 41\r\ncelebrity\r\n0\r\n1\r\n0.29\r\nFAL: 176, TRU: 71\r\ndanger\r\n0\r\n1\r\n0.30\r\nFAL: 172, TRU: 75\r\nanimals\r\n0\r\n1\r\n0.37\r\nFAL: 155, TRU: 92\r\nuse_sex\r\n0\r\n1\r\n0.27\r\nFAL: 181, TRU: 66\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nyear\r\n0\r\n1.00\r\n2010.19\r\n5.86\r\n2000\r\n2005\r\n2010\r\n2015.00\r\n2020\r\n▇▇▇▇▆\r\nview_count\r\n16\r\n0.94\r\n1407556.46\r\n11971111.01\r\n10\r\n6431\r\n41379\r\n170015.50\r\n176373378\r\n▇▁▁▁▁\r\nlike_count\r\n22\r\n0.91\r\n4146.03\r\n23920.40\r\n0\r\n19\r\n130\r\n527.00\r\n275362\r\n▇▁▁▁▁\r\ndislike_count\r\n22\r\n0.91\r\n833.54\r\n6948.52\r\n0\r\n1\r\n7\r\n24.00\r\n92990\r\n▇▁▁▁▁\r\nfavorite_count\r\n16\r\n0.94\r\n0.00\r\n0.00\r\n0\r\n0\r\n0\r\n0.00\r\n0\r\n▁▁▇▁▁\r\ncomment_count\r\n25\r\n0.90\r\n188.64\r\n986.46\r\n0\r\n1\r\n10\r\n50.75\r\n9190\r\n▇▁▁▁▁\r\ncategory_id\r\n16\r\n0.94\r\n19.32\r\n8.00\r\n1\r\n17\r\n23\r\n24.00\r\n29\r\n▃▁▂▆▇\r\nVariable type: POSIXct\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nmedian\r\nn_unique\r\npublished_at\r\n16\r\n0.94\r\n2006-02-06 10:02:36\r\n2021-01-27 13:11:29\r\n2013-01-31 09:13:55\r\n227\r\n\r\ndat_ref <- \"data from https://github.com/fivethirtyeight/superbowl-ads, via tidy tuesday\"\r\n\r\n\r\n\r\nlook at the data\r\n\r\n\r\nsb_raw %>% sample_n(10)\r\n\r\n\r\n# A tibble: 10 x 25\r\n    year brand  superbowl_ads_dot~ youtube_url  funny show_product_qu~\r\n   <dbl> <chr>  <chr>              <chr>        <lgl> <lgl>           \r\n 1  2007 Budwe~ https://superbowl~ https://www~ TRUE  TRUE            \r\n 2  2015 Budwe~ https://superbowl~ https://www~ FALSE TRUE            \r\n 3  2004 Bud L~ https://superbowl~ https://www~ TRUE  TRUE            \r\n 4  2006 Bud L~ https://superbowl~ https://www~ TRUE  FALSE           \r\n 5  2000 Budwe~ https://superbowl~ https://www~ TRUE  TRUE            \r\n 6  2009 Dorit~ https://superbowl~ https://www~ TRUE  TRUE            \r\n 7  2019 Kia    https://superbowl~ https://www~ FALSE FALSE           \r\n 8  2013 Budwe~ https://superbowl~ https://www~ FALSE TRUE            \r\n 9  2014 Coca-~ https://superbowl~ https://www~ FALSE TRUE            \r\n10  2009 Coca-~ https://superbowl~ https://www~ TRUE  TRUE            \r\n# ... with 19 more variables: patriotic <lgl>, celebrity <lgl>,\r\n#   danger <lgl>, animals <lgl>, use_sex <lgl>, id <chr>, kind <chr>,\r\n#   etag <chr>, view_count <dbl>, like_count <dbl>,\r\n#   dislike_count <dbl>, favorite_count <dbl>, comment_count <dbl>,\r\n#   published_at <dttm>, title <chr>, description <chr>,\r\n#   thumbnail <chr>, channel_title <chr>, category_id <dbl>\r\n\r\nclean the data a little bit\r\n\r\n\r\ndat <- \r\n  sb_raw %>% \r\n  mutate_if(is.character, snakecase::to_sentence_case) %>% \r\n  select(-favorite_count, -youtube_url) %>%\r\n  bind_cols(sb_raw[\"youtube_url\"],. ) %>%\r\n  rename(key = youtube_url)\r\n \r\n\r\ndat %>% colnames()\r\n\r\n\r\n [1] \"key\"                       \"year\"                     \r\n [3] \"brand\"                     \"superbowl_ads_dot_com_url\"\r\n [5] \"funny\"                     \"show_product_quickly\"     \r\n [7] \"patriotic\"                 \"celebrity\"                \r\n [9] \"danger\"                    \"animals\"                  \r\n[11] \"use_sex\"                   \"id\"                       \r\n[13] \"kind\"                      \"etag\"                     \r\n[15] \"view_count\"                \"like_count\"               \r\n[17] \"dislike_count\"             \"comment_count\"            \r\n[19] \"published_at\"              \"title\"                    \r\n[21] \"description\"               \"thumbnail\"                \r\n[23] \"channel_title\"             \"category_id\"              \r\n\r\nElbo method of guessing at clusters\r\n\r\n\r\ndat_4_clust <- \r\n  dat %>% \r\n  mutate_if(is.numeric, function(x){(x+1) %>% log10() %>%  scale() }) %>% \r\n  drop_na()\r\n\r\ndat_4_clust_2 <- \r\n  dat_4_clust %>% \r\n  select_if(function(x){is.logical(x) | is.numeric(x)})%>%\r\n  bind_cols(dat_4_clust[\"key\"],.) %>% \r\n  drop_na()\r\n\r\n\r\nkclusts <-\r\n  tibble(k = 1:12) %>%\r\n  mutate(\r\n    kclust = map(k, function(x){kmeans(x = select(dat_4_clust_2, -key), centers = x)})) %>% \r\n  mutate(glanced = map(kclust, glance))\r\n\r\n\r\nkclusts %>%\r\n  unnest(cols = c(glanced)) %>%\r\n  ggplot(aes(k, tot.withinss)) +\r\n  geom_line(alpha = 0.5, size = 1.2) +\r\n  geom_point(size = 2) + \r\n  labs(caption = dat_ref , title = \"Guess at cluster count via elbo method\", y = \"Total Withiness\", x = \"number of clusters\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n#best cluster ???\r\nfinal_clust <- kmeans(select(dat_4_clust_2, -key), centers = 7)\r\n\r\n\r\np <-\r\n  augment(final_clust, dat_4_clust_2) %>%\r\n  left_join(dat %>% select(brand, key), by = \"key\") %>% \r\n  ggplot(aes(dislike_count, like_count, fill = .cluster, name = brand)) +\r\n  geom_point(aes(size = view_count), shape = 21, color = \"black\") +\r\n  geom_text(aes(label  = brand), check_overlap = TRUE, hjust = \"right\") +\r\n  guides( fill = \"none\") + \r\n  labs(title = \"Clustering of NFL commercials\", y = \"likes (normalized)\", x = \"dislikes (normalized)\", \r\n       size = \"views (normalized)\",\r\n       caption = dat_ref )\r\n\r\np\r\n\r\n\r\n\r\n#pp <- ggplotly(p)\r\n#pp\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/tt_2021-03-02/distill-preview.png",
    "last_modified": "2023-03-07T12:29:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/firstPost/",
    "title": "Untitled",
    "description": "A new article created using the Distill format.",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill for R Markdown at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-03-07T12:29:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/template/",
    "title": "POST TITLE",
    "description": "A new post by hswerdfe",
    "author": [
      {
        "name": "Howard Swerdfeger",
        "url": "https://hswerdfe.github.io/docs/"
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\r\nimages\r\n\r\n\r\n\r\nload libraries\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(plotly)\r\n\r\n\r\n\r\ndo other stuff\r\nthis is text describing what I am doing\r\n\r\n\r\np <- \r\n iris %>% \r\n  ggplot(aes(x = Sepal.Width, y = Sepal.Length, color = Species)) + geom_point()\r\n\r\nggplotly(p)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/template/distill-preview.png",
    "last_modified": "2023-03-07T12:29:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to hswerdfe - data - dodo",
    "description": "Welcome to our new blog, hswerdfe - data - dodo. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-03-03",
    "categories": [],
    "contents": "\r\nfirst level header\r\nsome text\r\nseconde level header\r\nmore text\r\npictures\r\n\r\n\r\n\r\nreferences\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-03-07T12:29:30-05:00",
    "input_file": {}
  }
]

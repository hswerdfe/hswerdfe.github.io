---
title: "Mice for Imputation of Observations With Consecutive Related Dates"
description: |
  A new post by hswerdfe
author:
  - name: Howard Swerdfeger
    url: https://hswerdfe.github.io/docs/
    affiliation: Data DoDo
    affiliation_url: https://hswerdfe.github.io/docs/
date: "2021-04-09"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'svg')
```


## Motivation

Dataset with all of the following properties:

 * Observations with many dates which are near each other
 * Many cells with missing data
 * Bias in how the cells data are missing
 
Think of a dataset with some of these as dates: 

 1. Infection Date
 2. Lab Test Date
 3. Lab Result Date
 4. Onset Date
 5. Reported Date
 6. Recovery Date


## Load Libraries

First load the libraries needed, [mice](https://cran.r-project.org/web/packages/mice/index.html) is the library that actually does the imputation, while [ambient](https://cran.r-project.org/web/packages/ambient/index.html) is for creating a realistic looking fake dataset using perlin noise, and lemon is just for outputing tables.

```{r  echo=TRUE,  warning=FALSE, results='hide', message=FALSE}
library(tidyverse) # data manipulation
library(lubridate) # dates 
library(mice) # imputation
library(ambient) # perlin noise
library(lemon) # for table output
library(ggpubr) #for ggarrange
library(ggridges)

knit_print.data.frame <- lemon_print
theme_set(theme_minimal())

set.seed(as.integer(as.Date("2021-04-09")))
```

## Make some utility functions

These two function __noise_perlin_1D_pos__ and __sample_perlin__ are just used to help make the sample dataset we need, with dates that are missing but not missing at random.

```{r echo=TRUE}
#'
#' Returns 1D perlin Noise, and that noise is always above zero, vector length is size
#' examples:
#'    noise_perlin_1D_pos(50)
noise_perlin_1D_pos<- function(size, frequency = 0.007, ...){
  pn <- noise_perlin(c(size, 1), frequency = frequency, ...)[,1]
  pn - min(pn)  
}
```



```{r echo=TRUE}
#'
#' like base sample
#' samples from x size times, with prob given by prob, which defaults to perlin noise
#' 
sample_perlin <- function(x, size, replace = FALSE, ...,   
                          prob = noise_perlin_1D_pos(length(x), ...)){
  sample(x = x, size = size, prob = prob, replace = replace)
}
```

## Create Sample dataset

### Single time series

Create a single timeseries, that serves a the basis for the rest of the dataset.


```{r, echo=TRUE, dev = 'svg'}
n_rec = 10000
s_dt = as.Date("2020-01-01")
e_dt = as.Date("2021-01-01")
dt_a <- sample_perlin(s_dt:e_dt, n_rec, replace = TRUE, frequency = 0.007) %>% as.Date(origin = as.Date("1970-01-01")) %>% sort(
  
)


tibble(dt_a) %>% 
  ggplot(aes(x = dt_a)) +
  geom_density(color = "red", size = 1.5) +
  labs(title = "Density Plot of a base time series", x = "", y = "")
```


### Other Time series

All other timeseries are base on the initial timeseries. 
We add factors in, but we don't really use them.

```{r , echo=TRUE,   caption="Observations Dates are always close to each other.",render=lemon_print}




exposure_date <- dt_a + runif(n = n_rec, -21, -2) %>% round()
lab_date <- exposure_date + runif(n = n_rec, 0, +28)%>% round()
onset_date <- exposure_date + runif(n = n_rec, +2, +21)%>% round()
reported_date <- lab_date + runif(n = n_rec, +5, +10)%>% round()
recovery_date <- onset_date + runif(n = n_rec, +2, +21)%>% round()
factor_a <- sample(c("a","b","c","d"), n_rec, replace = TRUE)
factor_b <- sample(c("z","y","x","w"), n_rec, replace = TRUE)



dat_full <- 
  tibble(
    exposure_date, lab_date, onset_date, reported_date, recovery_date, factor_a, factor_b
  ) %>% 
  mutate(date_min = pmin(exposure_date, lab_date, onset_date, reported_date, recovery_date, na.rm = TRUE)) %>% 
  mutate(date_max = pmax(exposure_date, lab_date, onset_date, reported_date, recovery_date, na.rm = TRUE)) #%>% 
  #mutate(exposure_date = as.character(exposure_date))

dat_full %>% sample_n(7)

```

## Remove some values

```{r  , echo=TRUE}
remove_noise <- noise_perlin_1D_pos(n_rec, pertubation = 'fractal',  frequency = 0.0001) 



remove_noise %>% 
  tibble(value = ., index = 1:length(.)) %>%
  ggplot(aes(y = value, x = index)) + geom_line(color = "blue", size = 1.5) +
  theme_void() + 
  labs(title = "Probability density of cell being removed")
```


Randomly remove some values, but importantly the removal is not uniform across the whole dataset. 

Here the number removed is related to __size__ for example __date_b__ has about 1/3rd of its values removed.

```{r , echo=TRUE, out.width="100%",  caption="Some cells are hidden or set to NA.",render=lemon_print}

exposure_date[sample(x = 1:n_rec, size = n_rec/1.5, prob = remove_noise)] <- NA
lab_date[sample(x = 1:n_rec, size = n_rec/2, prob = remove_noise)] <- NA
onset_date[sample(x = 1:n_rec, size = 0.25*n_rec, prob = remove_noise)] <- NA
reported_date[sample(x = 1:n_rec, size = n_rec/1000, prob = remove_noise)] <- NA
recovery_date[sample(x = 1:n_rec, size = 0.90*n_rec, prob = remove_noise)] <- NA
factor_a[sample(x = 1:n_rec, size = n_rec/3, prob = remove_noise)] <- NA
factor_b[sample(x = 1:n_rec, size = n_rec/3, prob = remove_noise)] <- NA

dat <- 
  tibble(
    exposure_date, lab_date, onset_date, reported_date, recovery_date, factor_a, factor_b
  ) %>% 
  mutate(date_min = pmin(exposure_date, lab_date, onset_date, recovery_date, na.rm = TRUE)) %>% 
  mutate(date_max = pmax(exposure_date, lab_date, onset_date, recovery_date, na.rm = TRUE)) #%>% 
  #mutate(exposure_date = as.character(exposure_date))

dat %>% sample_n(7)

```

```{r}
dat %>% 
  select(everything()) %>%  # replace to your needs
  summarise_all(funs(100*sum(is.na(.))/n())) %>% 
  pivot_longer(everything())
```



### Compare missing with full data

Then compare the full dataset to the dataset with values removed. Note how badly some of the timeseries are represented, with the removed data. One of the worst is __recovery_date__. This is mainly because we removed 89 percent of the values.

```{r , echo=TRUE, out.width="100%"}
bind_rows(
dat %>% mutate(full = "Some Dates Removed"),
dat_full %>% mutate(full = "Full Dataset")
) %>%
  mutate_at(vars(matches("date")), as.Date) %>% 
  select(matches("date"), full) %>% 
  pivot_longer(cols = matches("date")) %>% 
  count(full, name, value) %>%
  filter(!is.na(value)) %>% 
  ggplot(aes(x = value, y = n, color = full)) + geom_line() + facet_wrap(vars(name)) + 
  labs(title = "Example Dataset", subtitle = "some dates are removed", y = "Count of Data", x = "", color = "") +
  theme(legend.position="bottom")


```
## Difference Between Dates

One thing we rely on is that the distribution of differences between the days is relatively consistent when data is removed, we can see that this is still the case.

If you have strong reason to suspect this is not the case, consider eliminating that date from the prediction.

```{r , echo=TRUE, out.width="100%"}
bind_rows(
  dat %>% mutate(full = "With Missing Data"),
  dat_full %>% mutate(full = "Full Dataset")
) %>% 
  mutate(delta_onset_reported = onset_date- reported_date) %>%
  ggplot(aes(x = delta_onset_reported, y = full, fill = full)) + 
  geom_density_ridges(alpha = 0.5) + 
  #facet_grid(rows = vars(full)) +
  guides(fill = FALSE) + 
  labs(x = "Days between Dates `e` and and `c`")
```

## Main Imputation  Function

Next is the main function to restore the data in a target date column. What this does:

 * where it can it shows finds the difference between the target date and all other dates
 * use __mice__ to fill in the missing deltas to the target
 * calculate the target where needed using new deltas and helper dates


I experimented a little with both my dataset and this fake data and found that predicting via __mice__ on the delta of the dates seems to produce more reasonable answers then predicting the actual number via __mice__ as often it seems a few dates will be way off with predicting __mice__ on the dates directly. even if the dates are normalized.

But I did no formal experiment. This was just my intuition, as these were my first two attempts and they did not have accurate results.

```{r  echo=TRUE,   warning=FALSE, results='hide', message=FALSE}


#'
#' Uses Mice to predict the delta between 'other dates' and target date.
#' When the 'target date' is missing estimate it from 'other dates'
#' Then takes average of predictions of 'target dates', and returns a vector of 'target dates' 
#' with all data filled in....
#' 
#' This method is useful when you have data with many dates
#' 
#' 
fill_summarize_date_data <- function(
                      dat,
                      post_grouping_nm,
                      predict_col_nm,
                      col_dt_match_ptrn ="date",
                      m = 3,
                      maxit = 50,
                      remove.collinear = FALSE, 
                      dt_origin_INTERNAL = as.Date("1970-01-01"),
                      simplify = TRUE,
                      ...
                    ){
  n_rec = nrow(dat)
  
  
  ###############################         
  #Get just date columns             
  dat_dt <- 
    dat %>% 
    select(matches(col_dt_match_ptrn)) %>%  
    mutate_all(as.Date) %>%  
    mutate_all(as.integer) 
  
  
  # Get the to and from columns
  pd_to = dat_dt[[predict_col_nm]]
  pd_from = dat_dt %>% select(-all_of(predict_col_nm))
  pd_from_mat <- as.matrix(pd_from)
  
  ###################################
  #do Mice on the deltas between columns
  detlta_dt_mice <- 
  map_dfc(colnames(pd_from), function(nm){
    pd_from[[nm]] - pd_to
  }) %>% set_names(colnames(pd_from)) %>% 
    mice::mice(m = m,      #number of multiple Imputations
         maxit = maxit,    #number of iterations
         remove.collinear=remove.collinear, ...)
  
  
  ############################
  #
  #Example:
  #
  # shows "m" estimate of the number of days between lab_date and target date, for each observation
  #
  # detlta_dt_mice$imp$lab_date
  #
  ####################################
  
  
  ###########################
  #get a bunch of predictions 
  #
  # One prediction based on each "helper column "m"
  dat_comp <- 
    map_dfc(1:m, function(im){
      
      #get ith prediction
      tmp <- mice::complete(data = detlta_dt_mice, im) %>% as_tibble()
      

      #in every helper column predict the target column when needed
      #from the value and the delta
      map_dfc(colnames(tmp), function(nm){
        from_x = pd_from[[nm]]
        dx = tmp[[nm]]
        ifelse(is.na(pd_to),# Only use the prediction if we need to, otherwise use the recored value
               from_x - dx,
               pd_to
        )
        
        
      }) %>% 
        setNames(colnames(tmp)) %>%
        mutate(., key = 1:nrow(.)) %>% 
        pivot_longer(., cols = matches(col_dt_match_ptrn)) %>%
        group_by(key) %>%
        summarise(value = mean(value, na.rm = TRUE)) %>% # AVERAGE each of the different types of dates
        pull(value) %>%
        as.Date(origin = dt_origin_INTERNAL)
    }) %>% set_names(paste0("n_", 1:m)) %>%
    bind_cols(dat[predict_col_nm])
  
  
    ##############################
    #strip out other stuff and just return one vector averaging all the predicted dates  
    dat_comp %>%
      mutate(., tmp_key = 1:nrow(.)) %>% 
      pivot_longer(., cols = starts_with("n_")) %>%
      group_by(tmp_key) %>%
      summarise(value = mean(value, na.rm = TRUE)) %>%
      ungroup() %>% 
      pull(value) %>% round() #%>% class()
}

```

### Imputation (Prediction)

Next do the prediction, important values are __m__ and __maxit__ which are both passed to `mice`. if you are interested in the individual entries I recommend increasing `maxit` and possibly __m__, at the cost of run time.

However, if you are going to __group by__ and __count__ then a lower number for __maxit__ is fine.


```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}

dat_guess <- 
  dat %>% 
  mutate(., exposure_date_guess = fill_summarize_date_data(., 
                          predict_col_nm = "exposure_date", 
                          col_dt_match_ptrn ="date",
                          m = 5, 
                          maxit = 100)
                          )# %>% sample_n(20)

```

__Note:__



 * __date_e_guess__ is fully filled out while __date_e__ is till missing some dates
 * __date_e__ and __date_e_guess__ are the same when they can be
 * __date_e_guess__ seems reasonable given the other dates in the observations.

```{r, echo=TRUE,  caption="New column date_e_guess.",render=lemon_print}
bind_cols(
dat_full %>% select(exposure_date),
dat_guess %>% select(exposure_date, exposure_date_guess) %>% rename(exposure_date_missing := exposure_date)
)%>% 
  mutate(guess_delta = exposure_date - exposure_date_guess) %>% 
  sample_n(10)
```

Looking at the distribution of errors in the guesses we see that there is little systemic error. if we are worried about individual observations then bringing in the standard deviation would be needed but looking at aggregate data can require less precision at the individual level. 

```{r, echo=TRUE}
bind_cols(
dat_full %>% select(exposure_date),
dat_guess %>% select(exposure_date, exposure_date_guess) %>% rename(exposure_date_missing := exposure_date)
)%>% 
  filter(is.na(exposure_date_missing)) %>% 
  mutate(guess_delta = exposure_date - exposure_date_guess) %>% 
  count(guess_delta) %>%
  ggplot(aes(x = guess_delta, y = n)) + geom_col(fill = "light yellow", alpha = 0.5, color = "black") + geom_vline(xintercept = 0, color = "Black", size = 1.5) +
  labs(y = "", x = "", title = "Distribution of Errors in Predicted Dates.")
```

## Compare Plot

Now we can see that predicting the missing dates in aggregate looks much better then just using the dates without imputation.

```{r echo=TRUE, out.width="100%"}

bind_rows(
  dat_full %>%
    mutate_at(vars(matches("date")), as.Date) %>% 
    select(matches("date")) %>% 
    pivot_longer(.,cols = colnames(.)) %>% 
    mutate(full = "full") 
  ,
  dat_guess %>% select(starts_with("exposure_date")) %>%
    pivot_longer(., cols = colnames(.)) %>%
    mutate(full = "missing") 
) %>% 
  mutate(type = paste0(name,"_", full)) %>%
  
  count(type, value, sort = T) %>%# view()
  filter(!is.na(value)) %>%
  filter(grepl('exposure_date', type) ) %>% 
  ggplot(aes(x = value, y= n, color = type)) + geom_line() +
  labs(title ="Comparison of full timeseries, missing and imputed timeseries", y = "Number of observations", x= "Date", color = "") +
    theme(legend.position="bottom")



```


```{r  echo=TRUE, out.width="100%"}

dat_comb <- 
bind_rows(
  dat_full %>%
    select(starts_with("exposure_date")) %>% 
    pivot_longer(.,cols = colnames(.)) %>% 
    mutate(full = "full") 
  ,
  dat_guess %>% select(starts_with("exposure_date")) %>%
    pivot_longer(., cols = colnames(.)) %>%
    mutate(full = "missing") 
) %>% 
  mutate(type = paste0(name,"_", full)) %>% 
  count(type, value, sort = T) %>% 
  filter(!is.na(value)) %>% 
  pivot_wider(names_from = type, values_from = n) %>% 
  mutate(error_in_counts = exposure_date_full- exposure_date_guess_missing) %>%
  mutate(error_in_counts_percent = error_in_counts/exposure_date_full)


dat_comb$error_in_counts %>% quantile(probs = c(0.1,0.25,0.5,0.75,0.90), na.rm = TRUE)

p1 <-
dat_comb %>%
  ggplot(aes(x = value, y = error_in_counts)) + geom_line() +
  labs(y ="Error between Actual counts and predicted counts", x = "Date", title = "Errors Look Random Which is Good.")
p2 <-
dat_comb %>% 
  ggplot(aes(x = error_in_counts)) + 
  geom_density(fill = "yellow", color = "black", alpha = 0.15) + 
  geom_vline(xintercept = 0 , color = "Black", size = 1.5) +
  #geom_boxplot(fill = "yellow", color = "black", alpha = 0.15) + 
  #geom_violin(fill = "yellow", color = "black", alpha = 0.15) + 
  labs(y = "Density", title = "Distribution Error in Count", x = "Count")


ggarrange(p1, p2)

```


# Conclusion

__Impute your Dates!__ ... Please...